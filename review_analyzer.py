import pandas as pd
import spacy
from collections import Counter
import logging
import sys
from typing import List, Dict, Set, Union, Tuple
import re
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
import pymorphy3
from concurrent.futures import ThreadPoolExecutor
import numpy as np
from datetime import datetime

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ YandexSpeller —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–∫–∏
try:
    from pyaspeller import YandexSpeller
    speller = YandexSpeller()
except ImportError:
    logging.warning("–ú–æ–¥—É–ª—å pyaspeller –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –±—É–¥–µ—Ç –æ—Ç–∫–ª—é—á–µ–Ω–∞.")
    speller = None

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π UTF-8 –∏ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —ç–º–æ–¥–∑–∏
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)],
    encoding='utf-8-sig'
)
logger = logging.getLogger(__name__)

# –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –∑–∞–º–µ–Ω–∞ —ç–º–æ–¥–∑–∏ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
def replace_emoji_for_logging(text: str) -> str:
    emoji_dict = {
        "üëç": "[–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π_—ç–º–æ–¥–∑–∏]",
        "üëå": "[–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π_—ç–º–æ–¥–∑–∏]",
        "üòä": "[–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è]",
        "üôÇ": "[–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è]",
        "üòç": "[–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è]",
        "üò¢": "[–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è]",
        "üò°": "[–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è]",
        "üò†": "[–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è]",
        "üòã": "[–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è]",
        "ÔºÅ": "–≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏–µ",
        "‚Äº": "–¥–≤–æ–π–Ω–æ–µ_–≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏–µ",
        "Ôºü": "–≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π_–∑–Ω–∞–∫",
        "üåª": "—Ü–≤–µ—Ç–æ–∫_—ç–º–æ–¥–∑–∏",
        "üöò": "–º–∞—à–∏–Ω–∞_—ç–º–æ–¥–∑–∏",
        "üëé": "[–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π_—ç–º–æ–¥–∑–∏]",
        "üòû": "[–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è]",
        "ü§î": "[–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è]",
        "üí°": "[–∏–¥–µ—è_—ç–º–æ–¥–∑–∏]"
    }
    for emoji, label in emoji_dict.items():
        text = text.replace(emoji, f" {label} ")
    return text.strip()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è spaCy –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
nlp = spacy.load("ru_core_news_sm", disable=["ner"])

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è pymorphy3 –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–ª–æ–≤
morph = pymorphy3.MorphAnalyzer()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
model_name = "seara/rubert-tiny2-russian-sentiment"
sentiment_analyzer = None
try:
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name)
    sentiment_analyzer = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
    logger.info(f"–ú–æ–¥–µ–ª—å {model_name} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")
except Exception as e:
    logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}: {e}. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å–ª–æ–≤–∞—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑.")
    sentiment_analyzer = None

# –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
SENTIMENT_DICT = {
    "—Ö–æ—Ä–æ—à–∏–π": ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", 0.7),
    "–æ—Ç–ª–∏—á–Ω—ã–π": ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", 0.9),
    "–ø—Ä–µ–∫—Ä–∞—Å–Ω—ã–π": ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", 0.8),
    "—É–¥–æ–±–Ω—ã–π": ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", 0.6),
    "–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π": ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", 0.7),
    "–±—ã—Å—Ç—Ä—ã–π": ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", 0.6),
    "–Ω–∞–¥–µ–∂–Ω—ã–π": ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", 0.7),
    "–≤–∫—É—Å–Ω—ã–π": ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", 0.7),
    "–∫—Ä–∞—Å–∏–≤—ã–π": ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", 0.6),
    "–ø—Ä–∏—è—Ç–Ω—ã–π": ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", 0.6),
    "–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω—ã–π": ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", 0.8),
    "—Å–≤–µ–∂–∏–π": ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", 0.6),
    "–≤–µ–∂–ª–∏–≤—ã–π": ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", 0.7),
    "–ø–ª–æ—Ö–æ–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.7),
    "—É–∂–∞—Å–Ω—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.9),
    "–¥–µ—Ñ–µ–∫—Ç–Ω—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.8),
    "–Ω–µ—É–¥–æ–±–Ω—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.6),
    "–º–µ–¥–ª–µ–Ω–Ω—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.6),
    "–Ω–µ–Ω–∞–¥–µ–∂–Ω—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.7),
    "–Ω–µ–≤–∫—É—Å–Ω—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.7),
    "—Å–ª–æ–º–∞–Ω–Ω—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.8),
    "–≥—Ä—è–∑–Ω—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.7),
    "–¥–æ—Ä–æ–≥–æ–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.7),
    "–±–µ–∑–≤–∫—É—Å–Ω—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.7),
    "–ø—Ä–æ–≥–æ—Ä–∫–ª—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.8),
    "–ø—Ä–æ—Ç—É—Ö—à–∏–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.9),
    "—Ç—Ä—ã–Ω–¥–µ—Ü": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.9),
    "—Ö—É–¥—à–∏–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.8),
    "—Ö–∞–º–æ–≤–∞—Ç—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.7),
    "–Ω–µ–∫–≤–∞–ª–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.7),
    "–≤—ã—Å–æ–∫–∏–π": ("–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ", 0.0),
    "–ø—Ä–æ—Å—Ç–æ": ("–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ", 0.0),
    "—Ä–∞–∑–≤–∏–≤–∞–µ—Ç—Å—è": ("–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ", 0.0),
    "—Ö—Ä–∞–Ω–∏—Ç—Å—è": ("–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ", 0.0),
    "—Ö—Ä–∞–Ω–µ–Ω–∏–∏": ("–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ", 0.0),
    "–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ": ("–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ", 0.0),
    "–ø–µ—Ä—Å–æ–Ω–∞–ª": ("–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ", 0.0),
    "—Ç–æ–ø–ª–∏–≤–æ": ("–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ", 0.0),
    "–Ω–µ—Ç": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.5),
    "–Ω–µ—Ç—É": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.5),
    "–æ—á–µ–Ω—å": ("—É—Å–∏–ª–∏—Ç–µ–ª—å", 1.5),
    "—Å–ª–µ–≥–∫–∞": ("–æ—Å–ª–∞–±–∏—Ç–µ–ª—å", 0.5),
    "—Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ": ("—É—Å–∏–ª–∏—Ç–µ–ª—å", 1.3),
    "–∞–±—Å–æ–ª—é—Ç–Ω–æ": ("—É—Å–∏–ª–∏—Ç–µ–ª—å", 1.4),
    "–Ω–µ–º–Ω–æ–≥–æ": ("–æ—Å–ª–∞–±–∏—Ç–µ–ª—å", 0.7),
    "–¥—Ä—è–Ω—å": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.8),
    "–æ–±–º–∞–Ω": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.7),
    "–≤–æ—Ä—É—é—Ç": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.9),
    "—Ö–º—É—Ä—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.6),
    "–Ω–µ–¥–æ—Å–º–æ—Ç—Ä": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.6),
    "–æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω–æ": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.9),
    "–≤–æ—Ä–æ–≤—Å—Ç–≤–æ": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.9),
    "–æ–±–º–∞–Ω—ã–≤–∞—é—Ç": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.8),
    "–Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.7),
    "–∂–µ—Å—Ç—å": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.7),
    "—Å—Ç—ã–¥–æ–±–µ–Ω—å": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.7),
    "–Ω–µ–¥–æ–ª–∏–≤": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.8),
    "—Ö–∞–º—Å—Ç–≤–æ": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.7),
    "–¥–æ—Ä–æ–≥–æ–≤–∞—Ç–æ": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.8),
    "–º–∞–ª–æ": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.6),
    "–º–∞–ª–µ–Ω—å–∫–∏–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.5),
    "–º–∞–ª–æ–≤–∞—Ç–æ": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.6),
    "—Å—Ç—Ä–∞–Ω–Ω—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.6),
    "–≤—ã—Å–æ–∫–∞—è": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.6),
    "–Ω–µ–ø–æ–Ω—è—Ç–Ω—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.6)
}

class ReviewAnalyzer:
    def __init__(self, use_preprocessing: bool = True, positive_threshold: int = 4, negative_threshold: int = 2):
        self.positive_keywords: Set[str] = set()
        self.negative_keywords: Set[str] = set()
        self.positive_threshold = positive_threshold
        self.negative_threshold = negative_threshold
        self.use_preprocessing = use_preprocessing
        self.invalid_phrases = {
            "–±–µ–∑–æ–±–∏–¥–Ω—ã–π –≤–∫—É—Å", "–∏–∑—è—â–Ω—ã–π –¥–µ–ª–∞—Ç—å—Å—è", "–≤–∫—É—Å–Ω—ã–π –±–ª–æ–∫", "–¥–æ–ø–æ—Ä—ã–≤–∞—Ç—å",
            "–ø—Ä–∏—Å–ª–∞—Ç—å —Ä–∞–∑–≤–∞–∫—É—É–º", "–≤–æ–∑–≤—Ä–∞—Ç –¥–µ–ª–∞—Ç—å –æ—Ç–∫–∞–∑—ã–≤–∞—Ç—å—Å—è", "–∫–æ—Å–∞—Ä—å –Ω–∞ –≤–µ—Ç–µ—Ä",
            "–ø–∞—Ö–Ω—É—Ç—å –±—ã–ª—å–≥–µ—Ç", "–∑–≤—ã–π —Ä–∞–∑", "–¥–≤–∞ —Ç—ã–∫", "–¥–æ—Ç–æ—à–Ω—ã–π –∞–∫–∫—É–º—É–ª—è—Ç–æ—Ä", "—à–µ–Ω—å 224"
        }
        self.sentiment_cache = {}
        self.domain_hints = ["—Ä—ã–±–∞", "–º—è—Å–æ", "–µ–¥–∞", "—ç–∫—Ä–∞–Ω", "–∫–∞–º–µ—Ä–∞", "–∫—Ä–µ–≤–µ—Ç–∫–∞", "–º–∏–Ω—Ç–∞–π", "–≥—Ä–µ–±–µ—à–æ–∫", "–≤–∫—É—Å", "–∫–∞—á–µ—Å—Ç–≤–æ", "—Ä–∞–∑–º–µ—Ä", "—Ç–æ–ø–ª–∏–≤–æ", "—Å–µ—Ä–≤–∏—Å", "–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ", "–ø–µ—Ä—Å–æ–Ω–∞–ª", "–±–∞–ª–ª—ã", "–ê–ó–°", "–±–µ–Ω–∑–∏–Ω", "–¥–∏–∑–µ–ª—å", "–∫–∞—Ä—Ç–∞", "—Ü–µ–Ω–∞", "–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ", "—Å–æ—Å—Ç–∞–≤", "—É–ø–∞–∫–æ–≤–∫–∞", "–∏–∫—Ä–∞"]
        self.review_data = []  # –î–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤ —Å –∏–º–µ–Ω–∞–º–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π

    def preprocess_text(self, text: str) -> str:
        if not self.use_preprocessing or not text.strip():
            return text
        if speller is None:
            logger.warning("pyaspeller –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç–∫–ª—é—á–µ–Ω–∞.")
            return text
        try:
            emoji_dict = {
                "üëç": "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π_—ç–º–æ–¥–∑–∏",
                "üëå": "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π_—ç–º–æ–¥–∑–∏",
                "üòä": "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è",
                "üôÇ": "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è",
                "üòç": "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è",
                "üò¢": "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è",
                "üò°": "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è",
                "üò†": "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è",
                "üòã": "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è",
                "ÔºÅ": "–≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏–µ",
                "‚Äº": "–¥–≤–æ–π–Ω–æ–µ_–≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏–µ",
                "Ôºü": "–≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π_–∑–Ω–∞–∫",
                "üåª": "—Ü–≤–µ—Ç–æ–∫_—ç–º–æ–¥–∑–∏",
                "üöò": "–º–∞—à–∏–Ω–∞_—ç–º–æ–¥–∑–∏"
            }
            for emoji, label in emoji_dict.items():
                text = text.replace(emoji, f" {label} ")
            corrected = speller.spelled(text)
            typo_corrections = {
                "–ø—Ä–∏ —Ö–∞—Ä–∏": "–ø—Ä–∏ —Ö—Ä–∞–Ω–µ–Ω–∏–∏",
                "—Ö–∞—Ä–∏": "—Ö—Ä–∞–Ω–µ–Ω–∏–∏",
            }
            for typo, correction in typo_corrections.items():
                corrected = corrected.replace(typo, correction)
            doc = nlp(corrected)
            normalized_tokens = []
            token_cache = {}
            for token in doc:
                if token.is_punct or token.is_stop:
                    normalized_tokens.append(token.text)
                    continue
                if token.text in token_cache:
                    normalized_tokens.append(token_cache[token.text])
                    continue
                parsed_word = morph.parse(token.text)[0]
                normal_form = parsed_word.normal_form
                token_cache[token.text] = normal_form
                normalized_tokens.append(normal_form)
            return " ".join(normalized_tokens).strip()
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –æ–ø–µ—á–∞—Ç–æ–∫: {str(e)}")
            return text

    def split_sentences(self, text: str) -> List[str]:
        text = re.sub(r'[!?.]+\)+|\)+|[:;]-?\)+', '.', text)
        text = re.sub(r'(\.+|\!+|\?+)', r'. ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        doc = nlp(text)
        sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]
        return sentences

    def split_mixed_sentence(self, sentence: str) -> List[str]:
        parts = re.split(r'\s+(–Ω–æ|–∞|–∏–ª–∏)\s+', sentence.lower())
        if len(parts) > 1:
            return [part.strip() for part in parts if part.strip()]
        return [sentence]

    def check_modifiers_with_dependencies(self, sentence: str) -> float:
        doc = nlp(sentence)
        sentiment_modifier = 1.0
        negation_count = 0
        intensifier = 1.0

        for token in doc:
            if token.lemma_ in ["–Ω–µ", "–Ω–µ—Ç", "–Ω–∏", "–µ–¥–≤–∞"] and token.dep_ in ["neg"]:
                head = token.head
                if head.pos_ in ["ADJ", "ADV", "VERB"]:
                    negation_count += 1
                for child in head.children:
                    if child.lemma_ in ["–Ω–µ", "–Ω–µ—Ç"] and child.dep_ in ["neg"]:
                        negation_count += 1
            elif token.lemma_ in ["–æ—á–µ–Ω—å", "–∫—Ä–∞–π–Ω–µ", "—Å–∏–ª—å–Ω–æ", "–∞–±—Å–æ–ª—é—Ç–Ω–æ", "—Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ"] and token.dep_ in ["advmod"]:
                intensifier = 1.5
            elif token.lemma_ in ["—Å–ª–µ–≥–∫–∞", "–Ω–µ–º–Ω–æ–≥–æ", "—á—É—Ç—å", "–µ–ª–µ"] and token.dep_ in ["advmod"]:
                intensifier = 0.5
            elif token.lemma_ in ["–≤—Ä—è–¥ –ª–∏", "–µ–¥–≤–∞ –ª–∏"] and token.dep_ in ["advmod"]:
                negation_count += 1

        if negation_count % 2 == 1:
            sentiment_modifier = -intensifier
        else:
            sentiment_modifier = intensifier
        logger.debug(f"–ú–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è '{replace_emoji_for_logging(sentence)}': {sentiment_modifier}, negation_count={negation_count}")
        return sentiment_modifier

    def analyze_sentiment_transformers(self, text: str) -> Tuple[str, float]:
        if not text.strip():
            return '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ', 0.0

        if text in self.sentiment_cache:
            return self.sentiment_cache[text]

        if sentiment_analyzer:
            try:
                result = sentiment_analyzer(text)[0]
                label = result['label'].lower()
                score = result['score']
                logger.info(f"–ú–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É–ª–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞ '{replace_emoji_for_logging(text)}': label={label}, score={score}")

                doc = nlp(text)
                token_count = len([token for token in doc if not token.is_punct])
                MIN_CONFIDENCE_THRESHOLD = 0.6

                has_negative = any(token.lemma_ in [k for k, (s, _) in SENTIMENT_DICT.items() if s == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ"] for token in doc)
                has_positive = any(token.lemma_ in [k for k, (s, _) in SENTIMENT_DICT.items() if s == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ"] for token in doc)
                negative_boost = 0
                if has_negative:
                    negative_words = [token.lemma_ for token in doc if SENTIMENT_DICT.get(token.lemma_, (None, None))[0] == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ"]
                    negative_boost = -0.7 * len(negative_words) * min(abs(SENTIMENT_DICT.get(word, (None, 0.0))[1]) for word in negative_words if SENTIMENT_DICT.get(word, (None, None))[0] == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ")
                    for i in range(len(negative_words) - 1):
                        if negative_words[i] in ["–æ—á–µ–Ω—å", "–º–µ–≥–∞"] and negative_words[i + 1] in ["–º–∞–ª–æ", "–¥–æ—Ä–æ–≥–æ", "–º–∞–ª–µ–Ω—å–∫–∏–π"]:
                            negative_boost *= 1.5

                if label == "positive" and score > MIN_CONFIDENCE_THRESHOLD:
                    base_score = score
                elif label == "negative" and score > MIN_CONFIDENCE_THRESHOLD:
                    base_score = -score
                elif label == "neutral":
                    if has_negative and negative_boost < -0.5:
                        base_score = negative_boost
                    elif has_positive:
                        base_score = 0.6
                    else:
                        base_score = 0.0
                else:
                    base_score = 0.0

                modifier = self.check_modifiers_with_dependencies(text)
                adjusted_score = base_score * modifier

                if "!" in text:
                    adjusted_score *= 1.2
                elif "..." in text or "?" in text:
                    adjusted_score *= 0.8

                if score < MIN_CONFIDENCE_THRESHOLD and abs(adjusted_score) < 0.5:
                    fallback_sentiment, fallback_score = self.fallback_sentiment_analysis(text)
                    combined_score = (score * 0.4) + (abs(fallback_score) * 0.6) * (-1 if fallback_sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ" else 1)
                    adjusted_score = combined_score

                sentiment = "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ" if adjusted_score > 0.4 else "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ" if adjusted_score < -0.4 else "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ"
                self.sentiment_cache[text] = (sentiment, adjusted_score)
                return sentiment, adjusted_score
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Å –º–æ–¥–µ–ª—å—é: {e}. –ü–µ—Ä–µ—Ö–æ–¥ –∫ —Å–ª–æ–≤–∞—Ä–Ω–æ–º—É –∞–Ω–∞–ª–∏–∑—É.")
                return self.fallback_sentiment_analysis(text)
        else:
            return self.fallback_sentiment_analysis(text)

    def fallback_sentiment_analysis(self, text: str) -> Tuple[str, float]:
        doc = nlp(text.lower())
        sentiment_scores = {"–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ": 0, "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ": 0}
        for token in doc:
            sentiment, score = self.get_sentiment(token.lemma_)
            if sentiment in ["–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ"]:
                sentiment_scores[sentiment] += score
        modifier = self.check_modifiers_with_dependencies(text)
        total_score = sentiment_scores["–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ"] + sentiment_scores["–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ"]
        total_score *= modifier
        if total_score > 0.4:
            return "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", total_score
        elif total_score < -0.4:
            return "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", total_score
        else:
            return "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ", 0.0

    def extract_aspects(self, sentence: str) -> List[Tuple[str, str, float, str]]:
        doc = nlp(sentence)
        aspects = []
        invalid_words = {"–æ—á–µ–Ω–∫", "–ø–æ—Ä–æ–±–æ–≤–∞—Ç—å", "—Å–ø–∞—Å–Ω–æ–≥–æ", "–∑–∞—Å–∫—É—á–∞—Ç—å—Å—è", "–¥–æ–±–æ–≤—Å—Ç–≤–æ", "—Ö–∞—Ä–∏"}
        MAX_ASPECT_LENGTH = 5

        if sentence.lower().strip() in ["–Ω–µ—Ç", "–Ω–µ—Ç—É"]:
            sentiment, score = self.get_sentiment("–Ω–µ—Ç")
            aspects.append(("–Ω–µ—Ç", sentiment, score, sentence))
            logger.info(f"–ò–∑–≤–ª–µ—á—ë–Ω –∞—Å–ø–µ–∫—Ç '–Ω–µ—Ç': —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å={sentiment}, —Å–∫–æ—Ä={score}")
            return aspects

        sentiment, score = self.analyze_sentiment_transformers(sentence)
        clauses = self.split_mixed_sentence(sentence)

        for clause in clauses:
            doc_clause = nlp(clause)
            for token in doc_clause:
                if (token.pos_ in ["NOUN", "ADJ", "VERB"] and token.lemma_ not in invalid_words and 
                    (token.lemma_ in self.domain_hints or token.lemma_ in [k for k, _ in SENTIMENT_DICT.items()])):
                    aspect_phrase = token.lemma_
                    modifiers = []
                    negation = False
                    children_count = 0
                    related_tokens = []

                    # –°–æ–±–∏—Ä–∞–µ–º —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã (–º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –∏ –∑–∞–≤–∏—Å–∏–º—ã–µ —Å–ª–æ–≤–∞)
                    for child in token.children:
                        if child.dep_ in ["amod", "compound", "advmod", "nmod", "obj"] and child.lemma_ not in invalid_words and children_count < MAX_ASPECT_LENGTH - 1:
                            modifiers.append(child.lemma_)
                            related_tokens.append(child)
                            children_count += 1
                        if child.lemma_ in ["–Ω–µ", "–Ω–µ—Ç", "–Ω–∏", "–µ–¥–≤–∞"] and child.dep_ in ["neg"]:
                            negation = True

                    # –ï—Å–ª–∏ —Ç–æ–∫–µ–Ω —Å–∞–º —è–≤–ª—è–µ—Ç—Å—è –∑–∞–≤–∏—Å–∏–º—ã–º, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ "—Ä–æ–¥–∏—Ç–µ–ª—è"
                    if token.dep_ in ["amod", "nmod", "obj"]:
                        parent = token.head
                        if parent.pos_ in ["NOUN", "VERB"] and parent.lemma_ not in invalid_words:
                            modifiers.append(parent.lemma_)
                            related_tokens.append(parent)

                    # –§–æ—Ä–º–∏—Ä—É–µ–º –∞—Å–ø–µ–∫—Ç –∏–∑ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
                    aspect_phrase = " ".join([mod for mod in modifiers if mod] + [token.lemma_]).strip()
                    if not aspect_phrase or aspect_phrase.lower() in self.invalid_phrases or len(aspect_phrase.split()) > MAX_ASPECT_LENGTH:
                        continue

                    aspect_sentiment = sentiment
                    aspect_score = score

                    if negation or any(mod in [k for k, (s, _) in SENTIMENT_DICT.items() if s == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ"] for mod in modifiers):
                        aspect_sentiment = "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ"
                        aspect_score = -abs(aspect_score) if aspect_score > 0 else aspect_score
                    elif any(mod in [k for k, (s, _) in SENTIMENT_DICT.items() if s == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ"] for mod in modifiers):
                        aspect_sentiment = "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ"
                        aspect_score = abs(aspect_score)

                    aspects.append((aspect_phrase, aspect_sentiment, aspect_score, clause))
                    logger.info(f"–ò–∑–≤–ª–µ—á—ë–Ω –∞—Å–ø–µ–∫—Ç: '{replace_emoji_for_logging(aspect_phrase)}', —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {aspect_sentiment}, —Å–∫–æ—Ä: {aspect_score}, –∏–∑ —Ç–µ–∫—Å—Ç–∞: '{clause}'")

            if not aspects and sentiment != "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ" and len(sentence.split()) <= MAX_ASPECT_LENGTH:
                aspects.append((sentence.strip(), sentiment, score, sentence))
                logger.info(f"–ò–∑–≤–ª–µ—á—ë–Ω –∞—Å–ø–µ–∫—Ç (–∏–∑ –≤—Å–µ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è): '{replace_emoji_for_logging(sentence.strip())}', —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {sentiment}, —Å–∫–æ—Ä: {score}")

        return aspects

    def analyze_review_sentences(self, review_text: str) -> List[Tuple[str, str, float, List[Tuple[str, str, float, str]]]]:
        review_text = self.preprocess_text(review_text)
        sentences = self.split_sentences(review_text)
        result = []
        for sentence in sentences:
            try:
                doc = nlp(sentence)
                clauses = []
                current_clause = []
                for token in doc:
                    current_clause.append(token.text)
                    if token.dep_ in ["cc", "punct"] and token.lemma_ in ["–∏", "–Ω–æ", "–∞", "–∏–ª–∏", ","]:
                        if current_clause:
                            clauses.append(" ".join(current_clause).strip())
                            current_clause = []
                if current_clause:
                    clauses.append(" ".join(current_clause).strip())

                for clause in clauses:
                    sentiment, score = self.analyze_sentiment_transformers(clause)
                    aspects = self.extract_aspects(clause)
                    result.append((clause, sentiment, score, aspects))
                    logger.info(f"–ê–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è '{replace_emoji_for_logging(clause)}': —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å={sentiment}, —Å–∫–æ—Ä={score}, –∞—Å–ø–µ–∫—Ç—ã={aspects}")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è '{replace_emoji_for_logging(sentence)}': {str(e)}")
                result.append((sentence, "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ", 0.0, []))
        return result

    def analyze_reviews(self, csv_path: str) -> Dict[str, str]:
        try:
            df = pd.read_csv(csv_path, encoding='utf-8')
            positive_count = 0
            negative_count = 0
            neutral_count = 0  # –î–æ–±–∞–≤–ª—è–µ–º —Å—á—ë—Ç—á–∏–∫ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤
            positive_aspects = Counter()
            negative_aspects = Counter()
            self.positive_keywords.clear()
            self.negative_keywords.clear()
            self.review_data = []  # –û—á–∏—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ—Ç–∑—ã–≤–æ–≤
            processed_texts_set = set()

            if all(col in df.columns for col in ['–î–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤–∞', '–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏', '–û—Ü–µ–Ω–∫–∞']):
                texts = [(str(row['–î–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤–∞']) if pd.notna(row['–î–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤–∞']) else "",
                         str(row['–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏']) if pd.notna(row['–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏']) else "",
                         int(row['–û—Ü–µ–Ω–∫–∞']) if pd.notna(row['–û—Ü–µ–Ω–∫–∞']) else 3,
                         str(row.get('–ò–º—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è', '–ê–Ω–æ–Ω–∏–º')))
                        for _, row in df.iterrows()]
                for pros_text, cons_text, rating, username in texts:
                    self.review_data.append({
                        'username': username,
                        'pros': pros_text,
                        'cons': cons_text,
                        'rating': rating,
                        'original_pros': pros_text,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø—Ä–∏–º–µ—Ä–æ–≤
                        'original_cons': cons_text
                    })
                    if pros_text.strip():
                        if pros_text.strip().lower() in ["–Ω–µ—Ç", "–Ω–µ—Ç—É"]:
                            negative_aspects["–Ω–µ—Ç"] += 1
                            self.negative_keywords.add("–Ω–µ—Ç")
                        else:
                            processed_texts_set.add(pros_text)
                            pros_sentences = self.analyze_review_sentences(pros_text)
                            for _, sentiment, _, aspects in pros_sentences:
                                if sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ":
                                    for aspect_phrase, aspect_sentiment, _, _ in aspects:
                                        if aspect_sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ":
                                            positive_aspects[aspect_phrase] += 1
                                            self.positive_keywords.add(aspect_phrase)
                                        elif aspect_sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ":
                                            negative_aspects[aspect_phrase] += 1
                                            self.negative_keywords.add(aspect_phrase)
                                elif sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ":
                                    for aspect_phrase, aspect_sentiment, _, _ in aspects:
                                        if aspect_sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ":
                                            negative_aspects[aspect_phrase] += 1
                                            self.negative_keywords.add(aspect_phrase)
                    if cons_text.strip():
                        if cons_text.strip().lower() in ["–Ω–µ—Ç", "–Ω–µ—Ç—É"]:
                            positive_aspects["–Ω–µ—Ç –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–æ–≤"] += 1
                            self.positive_keywords.add("–Ω–µ—Ç –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–æ–≤")
                        else:
                            processed_texts_set.add(cons_text)
                            cons_sentences = self.analyze_review_sentences(cons_text)
                            for _, sentiment, _, aspects in cons_sentences:
                                if sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ":
                                    for aspect_phrase, aspect_sentiment, _, _ in aspects:
                                        if aspect_sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ":
                                            positive_aspects[aspect_phrase] += 1
                                            self.positive_keywords.add(aspect_phrase)
                                elif sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ":
                                    for aspect_phrase, aspect_sentiment, _, _ in aspects:
                                        if aspect_sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ":
                                            negative_aspects[aspect_phrase] += 1
                                            self.negative_keywords.add(aspect_phrase)
                    if rating >= self.positive_threshold:
                        positive_count += 1
                    elif rating <= self.negative_threshold:
                        negative_count += 1
                    else:
                        neutral_count += 1

            elif all(col in df.columns for col in ['–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞', '–û—Ü–µ–Ω–∫–∞']):
                texts = [(str(row['–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞']), int(row['–û—Ü–µ–Ω–∫–∞']) if pd.notna(row['–û—Ü–µ–Ω–∫–∞']) else 3,
                         str(row.get('–ò–º—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è', '–ê–Ω–æ–Ω–∏–º')))
                        for _, row in df.iterrows()]
                for text, rating, username in texts:
                    self.review_data.append({
                        'username': username,
                        'pros': text,
                        'cons': '',
                        'rating': rating,
                        'original_pros': text,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
                        'original_cons': ''
                    })
                    if text.strip():
                        processed_texts_set.add(text)
                        sentences = self.analyze_review_sentences(text)
                        for _, sentiment, _, aspects in sentences:
                            if sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ":
                                for aspect_phrase, aspect_sentiment, _, _ in aspects:
                                    if aspect_sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ":
                                        positive_aspects[aspect_phrase] += 1
                                        self.positive_keywords.add(aspect_phrase)
                            elif sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ":
                                for aspect_phrase, aspect_sentiment, _, _ in aspects:
                                    if aspect_sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ":
                                        negative_aspects[aspect_phrase] += 1
                                        self.negative_keywords.add(aspect_phrase)
                    if rating >= self.positive_threshold:
                        positive_count += 1
                    elif rating <= self.negative_threshold:
                        negative_count += 1
                    else:
                        neutral_count += 1

            else:
                raise ValueError("CSV-—Ñ–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –ª–∏–±–æ —Å—Ç–æ–ª–±—Ü—ã '–î–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤–∞', '–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏' –∏ '–û—Ü–µ–Ω–∫–∞', –ª–∏–±–æ '–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞' –∏ '–û—Ü–µ–Ω–∫–∞'")

            common_aspects = set(positive_aspects.keys()).intersection(set(negative_aspects.keys()))
            for aspect in common_aspects:
                sentiment, _ = self.analyze_sentiment_transformers(aspect)
                if sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ":
                    del negative_aspects[aspect]
                    self.negative_keywords.discard(aspect)
                elif sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ":
                    del positive_aspects[aspect]
                    self.positive_keywords.discard(aspect)
                else:
                    pos_count = positive_aspects[aspect]
                    neg_count = negative_aspects[aspect]
                    if pos_count > neg_count:
                        del negative_aspects[aspect]
                        self.negative_keywords.discard(aspect)
                    else:
                        del positive_aspects[aspect]
                        self.positive_keywords.discard(aspect)

            main_positives = "\n".join([f"{aspect} ({count})" for aspect, count in positive_aspects.most_common(5)]) if positive_aspects else "–ù–µ—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤"
            main_negatives = "\n".join([f"{aspect} ({count})" for aspect, count in negative_aspects.most_common(5)]) if negative_aspects else "–ù–µ—Ç –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤"

            return {
                "–ü–ª—é—Å—ã": main_positives,
                "–ú–∏–Ω—É—Å—ã": main_negatives,
                "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ)": ", ".join(sorted(self.positive_keywords)[:10]),
                "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ)": ", ".join(sorted(self.negative_keywords)[:10]),
                "–û–±—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞": ", ".join(sorted(set(self.positive_keywords).union(self.negative_keywords))[:15]),
                "–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": str(positive_count),
                "–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": str(negative_count),
                "–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": str(neutral_count),  # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã
                "positive_aspects": positive_aspects,
                "negative_aspects": negative_aspects
            }
        except UnicodeDecodeError as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ {csv_path}: {str(e)}")
            return {
                "–ü–ª—é—Å—ã": "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏",
                "–ú–∏–Ω—É—Å—ã": "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏",
                "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ)": "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏",
                "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ)": "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏",
                "–û–±—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞": "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏",
                "–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏",
                "–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏",
                "–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏",
                "positive_aspects": Counter(),
                "negative_aspects": Counter()
            }
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Ñ–∞–π–ª–∞ {csv_path}: {str(e)}")
            return {
                "–ü–ª—é—Å—ã": "–û—à–∏–±–∫–∞",
                "–ú–∏–Ω—É—Å—ã": "–û—à–∏–±–∫–∞",
                "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ)": "–û—à–∏–±–∫–∞",
                "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ)": "–û—à–∏–±–∫–∞",
                "–û–±—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞": "–û—à–∏–±–∫–∞",
                "–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": "–û—à–∏–±–∫–∞",
                "–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": "–û—à–∏–±–∫–∞",
                "–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": "–û—à–∏–±–∫–∞",
                "positive_aspects": Counter(),
                "negative_aspects": Counter()
            }

    def aggregate_reviews(self, csv_paths: List[str]) -> Dict[str, str]:
        all_keywords: List[str] = []
        total_positive_count = 0
        total_negative_count = 0
        total_neutral_count = 0  # –î–æ–±–∞–≤–ª—è–µ–º —Å—á—ë—Ç—á–∏–∫ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤
        all_positive_aspects = Counter()
        all_negative_aspects = Counter()

        def process_single_file(csv_path):
            return self.analyze_reviews(csv_path)

        with ThreadPoolExecutor(max_workers=8) as executor:
            results = list(executor.map(process_single_file, csv_paths))

        for result in results:
            if result["–ü–ª—é—Å—ã"] != "–û—à–∏–±–∫–∞" and result["–ü–ª—é—Å—ã"] != "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏":
                positive_count = int(result["–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã"])
                negative_count = int(result["–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã"])
                neutral_count = int(result["–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã"])
                total_positive_count += positive_count
                total_negative_count += negative_count
                total_neutral_count += neutral_count
                all_positive_aspects.update(result["positive_aspects"])
                all_negative_aspects.update(result["negative_aspects"])
                all_keywords.extend(result["–û–±—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞"].split(", "))

        main_positives = "\n".join([f"{aspect} ({count})" for aspect, count in all_positive_aspects.most_common(5)]) if all_positive_aspects else "–ù–µ—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤"
        main_negatives = "\n".join([f"{aspect} ({count})" for aspect, count in all_negative_aspects.most_common(5)]) if all_negative_aspects else "–ù–µ—Ç –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤"
        top_keywords = ", ".join(sorted(set(all_keywords))[:15]) if all_keywords else "–ù–µ—Ç –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"

        return {
            "–ü–ª—é—Å—ã (–≤—Å–µ —Å–∞–π—Ç—ã)": main_positives,
            "–ú–∏–Ω—É—Å—ã (–≤—Å–µ —Å–∞–π—Ç—ã)": main_negatives,
            "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ, –≤—Å–µ —Å–∞–π—Ç—ã)": ", ".join(sorted(self.positive_keywords)[:10]),
            "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ, –≤—Å–µ —Å–∞–π—Ç—ã)": ", ".join(sorted(self.negative_keywords)[:10]),
            "–û–±—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–≤—Å–µ —Å–∞–π—Ç—ã)": top_keywords,
            "–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã (–≤—Å–µ —Å–∞–π—Ç—ã)": str(total_positive_count),
            "–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã (–≤—Å–µ —Å–∞–π—Ç—ã)": str(total_negative_count),
            "–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã (–≤—Å–µ —Å–∞–π—Ç—ã)": str(total_neutral_count),
            "positive_aspects": all_positive_aspects,
            "negative_aspects": all_negative_aspects
        }

    def get_sentiment(self, word: str) -> Tuple[str, float]:
        return SENTIMENT_DICT.get(word, ("–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ", 0.0))

    def update_sentiment_dict(self, word: str, sentiment: str, score: float):
        if word in SENTIMENT_DICT:
            current_sentiment, current_score = SENTIMENT_DICT[word]
            new_score = (current_score + score) / 2
            SENTIMENT_DICT[word] = (sentiment, new_score)
        else:
            SENTIMENT_DICT[word] = (sentiment, score)
        logger.info(f"–û–±–Ω–æ–≤–ª—ë–Ω —Å–ª–æ–≤–∞—Ä—å —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏: {word} -> {SENTIMENT_DICT[word]}")

    def generate_detailed_report(self, analysis_result: Dict[str, str], product_name: str = "–ø—Ä–æ–¥—É–∫—Ç") -> str:
        positive_aspects = analysis_result["positive_aspects"]
        negative_aspects = analysis_result["negative_aspects"]
        positive_count = int(analysis_result["–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã"])
        negative_count = int(analysis_result["–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã"])
        neutral_count = int(analysis_result["–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã"])
        total_reviews = positive_count + negative_count + neutral_count

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –æ—Ç—á—ë—Ç–∞
        current_time = datetime.now().strftime("%d.%m.%Y %H:%M")
        report = f"–ù–∏–∫–∏—Ç–∞ –ß–µ–ª—ã—à–µ–≤, [{current_time}]\n\n"

        # –ö–ª—é—á–µ–≤—ã–µ –ø–ª—é—Å—ã
        report += "–ö–ª—é—á–µ–≤—ã–µ –ø–ª—é—Å—ã, –æ—Ç–º–µ—á–µ–Ω–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏:\n\n"
        for aspect, count in positive_aspects.most_common(5):
            # –ò—â–µ–º –ø—Ä–∏–º–µ—Ä—ã –∏–∑ –æ—Ç–∑—ã–≤–æ–≤
            examples = []
            aspect_words = set(aspect.lower().split())  # –†–∞–∑–±–∏–≤–∞–µ–º –∞—Å–ø–µ–∫—Ç –Ω–∞ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
            for review in self.review_data:
                pros_text = review['original_pros'].lower() if review['original_pros'] else ''
                if all(word in pros_text for word in aspect_words):  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ —Å–ª–æ–≤–∞ –∞—Å–ø–µ–∫—Ç–∞ –µ—Å—Ç—å –≤ —Ç–µ–∫—Å—Ç–µ
                    examples.append(f'"{review["original_pros"]}" ({review["username"]})')
                    if len(examples) >= 2:
                        break
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∞—Å–ø–µ–∫—Ç –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
            formatted_aspect = ' '.join(word.capitalize() for word in aspect.split())
            report += f"{formatted_aspect}\n"
            report += f"–ß–∞—Å—Ç–æ —É–ø–æ–º–∏–Ω–∞–µ—Ç—Å—è ({count} —Ä–∞–∑). –ü—Ä–∏–º–µ—Ä—ã: {', '.join(examples) if examples else '–ü—Ä–∏–º–µ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.'}\n\n"

        # –ö–ª—é—á–µ–≤—ã–µ –º–∏–Ω—É—Å—ã
        report += "\n\n–ö–ª—é—á–µ–≤—ã–µ –º–∏–Ω—É—Å—ã, –æ—Ç–º–µ—á–µ–Ω–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏:\n\n"
        for aspect, count in negative_aspects.most_common(5):
            examples = []
            aspect_words = set(aspect.lower().split())
            for review in self.review_data:
                pros_text = review['original_pros'].lower() if review['original_pros'] else ''
                cons_text = review['original_cons'].lower() if review['original_cons'] else ''
                review_text = pros_text + ' ' + cons_text
                if all(word in review_text for word in aspect_words):
                    examples.append(f'"{review["original_pros"]}" ({review["username"]})')
                    if len(examples) >= 2:
                        break
            formatted_aspect = ' '.join(word.capitalize() for word in aspect.split())
            report += f"{formatted_aspect}\n"
            report += f"–ß–∞—Å—Ç–æ —É–ø–æ–º–∏–Ω–∞–µ—Ç—Å—è ({count} —Ä–∞–∑). –ü—Ä–∏–º–µ—Ä—ã: {', '.join(examples) if examples else '–ü—Ä–∏–º–µ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.'}\n\n"

        # –û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ –æ—Ü–µ–Ω–æ–∫
        report += "\n\n–û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ –æ—Ü–µ–Ω–æ–∫:\n\n"
        positive_percentage = (positive_count / total_reviews * 100) if total_reviews > 0 else 0
        negative_percentage = (negative_count / total_reviews * 100) if total_reviews > 0 else 0
        neutral_percentage = (neutral_count / total_reviews * 100) if total_reviews > 0 else 0
        average_rating = sum(review['rating'] for review in self.review_data) / total_reviews if total_reviews > 0 else 0
        report += f"–°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞: {average_rating:.1f} –∏–∑ 5. –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã (–æ—Ü–µ–Ω–∫–∏ 4-5) —Å–æ—Å—Ç–∞–≤–ª—è—é—Ç –æ–∫–æ–ª–æ {positive_percentage:.1f}% ({positive_count} –æ—Ç–∑—ã–≤–æ–≤), "
        report += f"–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ (–æ—Ü–µ–Ω–∫–∏ 1-2) ‚Äî –æ–∫–æ–ª–æ {negative_percentage:.1f}% ({negative_count} –æ—Ç–∑—ã–≤–æ–≤), "
        report += f"–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ (–æ—Ü–µ–Ω–∫–∞ 3) ‚Äî –æ–∫–æ–ª–æ {neutral_percentage:.1f}% ({neutral_count} –æ—Ç–∑—ã–≤–æ–≤).\n\n"

        # –î–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥
        report += f"\n\n–î–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥:\n\n"
        top_positive_aspects = [aspect for aspect, _ in positive_aspects.most_common(3)]
        top_negative_aspects = [aspect for aspect, _ in negative_aspects.most_common(3)]
        
        if positive_percentage > negative_percentage:
            report += f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –≤ —Ü–µ–ª–æ–º –¥–æ–≤–æ–ª—å–Ω—ã {product_name}. –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –≤–∫–ª—é—á–∞—é—Ç {', '.join(top_positive_aspects).lower()}. "
            report += f"–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã —Å–æ—Å—Ç–∞–≤–ª—è—é—Ç {positive_percentage:.1f}%, —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—ë–Ω–Ω–æ—Å—Ç—å –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞.\n\n"
            report += f"–û–¥–Ω–∞–∫–æ –µ—Å—Ç—å –∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏. –û—Å–Ω–æ–≤–Ω—ã–µ –∏–∑ –Ω–∏—Ö ‚Äî {', '.join(top_negative_aspects).lower()}. "
            report += f"–≠—Ç–∏ –∞—Å–ø–µ–∫—Ç—ã —É–ø–æ–º–∏–Ω–∞—é—Ç—Å—è —Ä–µ–∂–µ –∏ —Å–æ—Å—Ç–∞–≤–ª—è—é—Ç {negative_percentage:.1f}% –æ—Ç–∑—ã–≤–æ–≤, –Ω–æ –º–æ–≥—É—Ç –ø–æ–≤–ª–∏—è—Ç—å –Ω–∞ –æ–±—â–µ–µ –≤–ø–µ—á–∞—Ç–ª–µ–Ω–∏–µ.\n\n"
        else:
            report += f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –≤—ã—Ä–∞–∂–∞—é—Ç —Å–º–µ—à–∞–Ω–Ω—ã–µ –≤–ø–µ—á–∞—Ç–ª–µ–Ω–∏—è –æ {product_name}. –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –≤–∫–ª—é—á–∞—é—Ç {', '.join(top_positive_aspects).lower()}. "
            report += f"–û–¥–Ω–∞–∫–æ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã —Å–æ—Å—Ç–∞–≤–ª—è—é—Ç –ª–∏—à—å {positive_percentage:.1f}%, —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–µ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–∏–µ.\n\n"
            report += f"–ù–∞–∏–±–æ–ª—å—à–∏–µ –ø—Ä–æ–±–ª–µ–º—ã —Å–≤—è–∑–∞–Ω—ã —Å {', '.join(top_negative_aspects).lower()}. "
            report += f"–≠—Ç–∏ –∞—Å–ø–µ–∫—Ç—ã —É–ø–æ–º–∏–Ω–∞—é—Ç—Å—è —á–∞—â–µ –∏ –æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ, —á—Ç–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç—Å—è –ø—Ä–µ–æ–±–ª–∞–¥–∞–Ω–∏–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤ ({negative_percentage:.1f}%).\n\n"

        # –ò—Ç–æ–≥
        report += f"\n\n–ò—Ç–æ–≥: {product_name} –ø–æ–ª—É—á–∞–µ—Ç —Å—Ä–µ–¥–Ω—é—é –æ—Ü–µ–Ω–∫—É {average_rating:.1f} –∏–∑ 5, "
        if average_rating >= 4.0:
            report += f"—á—Ç–æ —Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤—É–µ—Ç –æ –≤—ã—Å–æ–∫–æ–º —É—Ä–æ–≤–Ω–µ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—ë–Ω–Ω–æ—Å—Ç–∏. "
        elif average_rating >= 3.0:
            report += f"—á—Ç–æ –æ—Ç—Ä–∞–∂–∞–µ—Ç —É–º–µ—Ä–µ–Ω–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ –ø—Ä–æ–¥—É–∫—Ç–∞. "
        else:
            report += f"—á—Ç–æ –æ—Ç—Ä–∞–∂–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã —Å –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ–º –ø—Ä–æ–¥—É–∫—Ç–∞. "
        report += f"–ö–æ–º–ø–∞–Ω–∏–∏ —Å—Ç–æ–∏—Ç —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏—Ç—å—Å—è –Ω–∞ —É–ª—É—á—à–µ–Ω–∏–∏ {', '.join([aspect for aspect, _ in negative_aspects.most_common(2)]).lower()}, —á—Ç–æ–±—ã –ø–æ–≤—ã—Å–∏—Ç—å –ª–æ—è–ª—å–Ω–æ—Å—Ç—å –∫–ª–∏–µ–Ω—Ç–æ–≤.\n"

        return report