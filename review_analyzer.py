import pandas as pd
import spacy
from collections import Counter
import logging
import sys
from typing import List, Dict, Set, Union, Tuple
import re
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
import pymorphy3
from multiprocessing import Pool
import numpy as np
from datetime import datetime
import json
from collections import OrderedDict

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ YandexSpeller
try:
    from pyaspeller import YandexSpeller
    speller = YandexSpeller()
except ImportError:
    speller = None
    logging.warning("–ú–æ–¥—É–ª—å pyaspeller –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –±—É–¥–µ—Ç –æ—Ç–∫–ª—é—á–µ–Ω–∞.")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.DEBUG,  # –ò–∑–º–µ–Ω–∏–º —É—Ä–æ–≤–µ–Ω—å –Ω–∞ DEBUG, —á—Ç–æ–±—ã –≤–∏–¥–µ—Ç—å –±–æ–ª—å—à–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),  # –í—ã–≤–æ–¥ –≤ –∫–æ–Ω—Å–æ–ª—å
        logging.FileHandler('review_analyzer.log', encoding='utf-8')  # –í—ã–≤–æ–¥ –≤ —Ñ–∞–π–ª
    ]
)
logger = logging.getLogger(__name__)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è spaCy –∏ pymorphy3
nlp = spacy.load("ru_core_news_sm", disable=["ner"])
morph = pymorphy3.MorphAnalyzer()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
model_name = "seara/rubert-tiny2-russian-sentiment"
sentiment_analyzer = None
try:
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name)
    sentiment_analyzer = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer, device=-1)
    logger.info(f"–ú–æ–¥–µ–ª—å {model_name} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")
except Exception as e:
    logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}: {e}. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å–ª–æ–≤–∞—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑.")
    sentiment_analyzer = None

# –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
SENTIMENT_DICT = {
    "—Ö–æ—Ä–æ—à–∏–π": ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", 0.7), "–æ—Ç–ª–∏—á–Ω—ã–π": ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", 0.9), "–ø—Ä–µ–∫—Ä–∞—Å–Ω—ã–π": ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", 0.8),
    "—É–¥–æ–±–Ω—ã–π": ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", 0.6), "–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π": ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", 0.7), "–±—ã—Å—Ç—Ä—ã–π": ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", 0.6),
    "–Ω–∞–¥–µ–∂–Ω—ã–π": ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", 0.7), "–≤–∫—É—Å–Ω—ã–π": ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", 0.7), "–∫—Ä–∞—Å–∏–≤—ã–π": ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", 0.6),
    "–ø—Ä–∏—è—Ç–Ω—ã–π": ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", 0.6), "–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω—ã–π": ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", 0.8), "—Å–≤–µ–∂–∏–π": ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", 0.6),
    "–≤–µ–∂–ª–∏–≤—ã–π": ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", 0.7), "–ø–ª–æ—Ö–æ–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.7), "—É–∂–∞—Å–Ω—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.9),
    "–¥–µ—Ñ–µ–∫—Ç–Ω—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.8), "–Ω–µ—É–¥–æ–±–Ω—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.6), "–º–µ–¥–ª–µ–Ω–Ω—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.6),
    "–Ω–µ–Ω–∞–¥–µ–∂–Ω—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.7), "–Ω–µ–≤–∫—É—Å–Ω—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.7), "—Å–ª–æ–º–∞–Ω–Ω—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.8),
    "–≥—Ä—è–∑–Ω—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.7), "–¥–æ—Ä–æ–≥–æ–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.7), "–±–µ–∑–≤–∫—É—Å–Ω—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.7),
    "–ø—Ä–æ–≥–æ—Ä–∫–ª—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.8), "–ø—Ä–æ—Ç—É—Ö—à–∏–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.9), "—Ç—Ä—ã–Ω–¥–µ—Ü": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.9),
    "—Ö—É–¥—à–∏–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.8), "—Ö–∞–º–æ–≤–∞—Ç—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.7), "–Ω–µ–∫–≤–∞–ª–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.7),
    "–≤—ã—Å–æ–∫–∏–π": ("–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ", 0.0), "–ø—Ä–æ—Å—Ç–æ": ("–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ", 0.0), "—Ä–∞–∑–≤–∏–≤–∞–µ—Ç—Å—è": ("–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ", 0.0),
    "—Ö—Ä–∞–Ω–∏—Ç—Å—è": ("–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ", 0.0), "—Ö—Ä–∞–Ω–µ–Ω–∏–∏": ("–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ", 0.0), "–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ": ("–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ", 0.0),
    "–ø–µ—Ä—Å–æ–Ω–∞–ª": ("–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ", 0.0), "—Ç–æ–ø–ª–∏–≤–æ": ("–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ", 0.0), "–Ω–µ—Ç": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.5),
    "–Ω–µ—Ç—É": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.5), "–æ—á–µ–Ω—å": ("—É—Å–∏–ª–∏—Ç–µ–ª—å", 1.5), "—Å–ª–µ–≥–∫–∞": ("–æ—Å–ª–∞–±–∏—Ç–µ–ª—å", 0.5),
    "—Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ": ("—É—Å–∏–ª–∏—Ç–µ–ª—å", 1.3), "–∞–±—Å–æ–ª—é—Ç–Ω–æ": ("—É—Å–∏–ª–∏—Ç–µ–ª—å", 1.4), "–Ω–µ–º–Ω–æ–≥–æ": ("–æ—Å–ª–∞–±–∏—Ç–µ–ª—å", 0.7),
    "–¥—Ä—è–Ω—å": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.8), "–æ–±–º–∞–Ω": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.7), "–≤–æ—Ä—É—é—Ç": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.9),
    "—Ö–º—É—Ä—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.6), "–Ω–µ–¥–æ—Å–º–æ—Ç—Ä": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.6), "–æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω–æ": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.9),
    "–≤–æ—Ä–æ–≤—Å—Ç–≤–æ": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.9), "–æ–±–º–∞–Ω—ã–≤–∞—é—Ç": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.8), "–Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.7),
    "–∂–µ—Å—Ç—å": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.7), "—Å—Ç—ã–¥–æ–±–µ–Ω—å": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.7), "–Ω–µ–¥–æ–ª–∏–≤": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.8),
    "—Ö–∞–º—Å—Ç–≤–æ": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.7), "–¥–æ—Ä–æ–≥–æ–≤–∞—Ç–æ": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.8), "–º–∞–ª–æ": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.6),
    "–º–∞–ª–µ–Ω—å–∫–∏–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.5), "–º–∞–ª–æ–≤–∞—Ç–æ": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.6), "—Å—Ç—Ä–∞–Ω–Ω—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.6),
    "–≤—ã—Å–æ–∫–∞—è": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.6), "–Ω–µ–ø–æ–Ω—è—Ç–Ω—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.6)
}

class ReviewAnalyzer:
    def __init__(self, use_preprocessing: bool = True, positive_threshold: int = 4, negative_threshold: int = 2):
        self.positive_keywords: Set[str] = set()
        self.negative_keywords: Set[str] = set()
        self.positive_threshold = positive_threshold
        self.negative_threshold = negative_threshold
        self.use_preprocessing = use_preprocessing
        self.invalid_phrases = {
            "–±–µ–∑–æ–±–∏–¥–Ω—ã–π –≤–∫—É—Å", "–∏–∑—è—â–Ω—ã–π –¥–µ–ª–∞—Ç—å—Å—è", "–≤–∫—É—Å–Ω—ã–π –±–ª–æ–∫", "–¥–æ–ø–æ—Ä—ã–≤–∞—Ç—å",
            "–ø—Ä–∏—Å–ª–∞—Ç—å —Ä–∞–∑–≤–∞–∫—É—É–º", "–≤–æ–∑–≤—Ä–∞—Ç –¥–µ–ª–∞—Ç—å –æ—Ç–∫–∞–∑—ã–≤–∞—Ç—å—Å—è", "–∫–æ—Å–∞—Ä—å –Ω–∞ –≤–µ—Ç–µ—Ä",
            "–ø–∞—Ö–Ω—É—Ç—å –±—ã–ª—å–≥–µ—Ç", "–∑–≤—ã–π —Ä–∞–∑", "–¥–≤–∞ —Ç—ã–∫", "–¥–æ—Ç–æ—à–Ω—ã–π –∞–∫–∫—É–º—É–ª—è—Ç–æ—Ä", "—à–µ–Ω—å 224"
        }
        self.sentiment_cache = OrderedDict()
        self.max_cache_size = 10000
        self.domain_hints = [
            "—Ä—ã–±–∞", "–º—è—Å–æ", "–µ–¥–∞", "—ç–∫—Ä–∞–Ω", "–∫–∞–º–µ—Ä–∞", "–∫—Ä–µ–≤–µ—Ç–∫–∞", "–º–∏–Ω—Ç–∞–π", "–≥—Ä–µ–±–µ—à–æ–∫", "–≤–∫—É—Å", "–∫–∞—á–µ—Å—Ç–≤–æ", "—Ä–∞–∑–º–µ—Ä",
            "—Ç–æ–ø–ª–∏–≤–æ", "—Å–µ—Ä–≤–∏—Å", "–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ", "–ø–µ—Ä—Å–æ–Ω–∞–ª", "–±–∞–ª–ª—ã", "–ê–ó–°", "–±–µ–Ω–∑–∏–Ω", "–¥–∏–∑–µ–ª—å", "–∫–∞—Ä—Ç–∞", "—Ü–µ–Ω–∞",
            "–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ", "—Å–æ—Å—Ç–∞–≤", "—É–ø–∞–∫–æ–≤–∫–∞", "–∏–∫—Ä–∞"
        ]
        self.review_data = []
        self.stop_words = set(["–∏", "–≤", "–Ω–∞", "—Å", "–ø–æ", "–¥–ª—è", "–∫", "—É", "–∏–∑", "–æ", "–∞", "–Ω–æ", "–∏–ª–∏"])
        self.emoji_dict = {
            "üëç": "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π_—ç–º–æ–¥–∑–∏", "üëå": "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π_—ç–º–æ–¥–∑–∏", "üòä": "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è",
            "üôÇ": "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è", "üòç": "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è", "üò¢": "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è",
            "üò°": "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è", "üò†": "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è", "üòã": "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è",
            "ÔºÅ": "–≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏–µ", "‚Äº": "–¥–≤–æ–π–Ω–æ–µ_–≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏–µ", "Ôºü": "–≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π_–∑–Ω–∞–∫",
            "üåª": "—Ü–≤–µ—Ç–æ–∫_—ç–º–æ–¥–∑–∏", "üöò": "–º–∞—à–∏–Ω–∞_—ç–º–æ–¥–∑–∏", "üëé": "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π_—ç–º–æ–¥–∑–∏",
            "üòû": "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è", "ü§î": "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è", "üí°": "–∏–¥–µ—è_—ç–º–æ–¥–∑–∏"
        }
        self.load_cache()

    def save_cache(self):
        with open("sentiment_cache.json", "w", encoding="utf-8") as f:
            json.dump(dict(self.sentiment_cache), f, ensure_ascii=False)

    def load_cache(self):
        try:
            with open("sentiment_cache.json", "r", encoding="utf-8") as f:
                self.sentiment_cache = OrderedDict(json.load(f))
        except FileNotFoundError:
            self.sentiment_cache = OrderedDict()

    def log_safe(self, text: str) -> str:
        return self.replace_emojis(text)

    def replace_emojis(self, text: str) -> str:
        for emoji, label in self.emoji_dict.items():
            text = text.replace(emoji, f" {label} ")
        return text.strip()

    def preprocess_text(self, text: str) -> str:
        if not self.use_preprocessing or not text.strip():
            return text
        if speller is None:
            logger.warning("pyaspeller –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç–∫–ª—é—á–µ–Ω–∞.")
            return text
        try:
            text = self.replace_emojis(text)
            corrected = speller.spelled(text)
            typo_corrections = {"–ø—Ä–∏ —Ö–∞—Ä–∏": "–ø—Ä–∏ —Ö—Ä–∞–Ω–µ–Ω–∏–∏", "—Ö–∞—Ä–∏": "—Ö—Ä–∞–Ω–µ–Ω–∏–∏"}
            for typo, correction in typo_corrections.items():
                corrected = corrected.replace(typo, correction)
            doc = nlp(corrected)
            normalized_tokens = [
                morph.parse(token.text)[0].normal_form
                for token in doc
                if not token.is_punct and not token.is_stop and token.text.lower() not in self.stop_words
            ]
            return " ".join(normalized_tokens).strip()
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–∞: {str(e)}")
            return text

    def split_sentences(self, text: str) -> List[str]:
        text = re.sub(r'[!?.]+\)+|\)+|[:;]-?\)+', '.', text)
        text = re.sub(r'(\.+|\!+|\?+)', r'. ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        doc = nlp(text)
        return [sent.text.strip() for sent in doc.sents if sent.text.strip()]

    def split_mixed_sentence(self, sentence: str) -> List[str]:
        parts = re.split(r'\s+(–Ω–æ|–∞|–∏–ª–∏|—Ö–æ—Ç—è|–∑–∞—Ç–æ)\s+', sentence.lower())
        return [part.strip() for part in parts if part.strip()] if len(parts) > 1 else [sentence]

    def check_modifiers_with_dependencies(self, sentence: str) -> float:
        doc = nlp(sentence)
        sentiment_modifier = 1.0
        negation_count = 0
        intensifier = 1.0
        for token in doc:
            if token.lemma_ in ["–Ω–µ", "–Ω–µ—Ç", "–Ω–∏", "–µ–¥–≤–∞"] and token.dep_ in ["neg"]:
                negation_count += 1
            elif token.lemma_ in ["–æ—á–µ–Ω—å", "–∫—Ä–∞–π–Ω–µ", "—Å–∏–ª—å–Ω–æ", "–∞–±—Å–æ–ª—é—Ç–Ω–æ", "—Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ"] and token.dep_ in ["advmod"]:
                intensifier = 1.5
            elif token.lemma_ in ["—Å–ª–µ–≥–∫–∞", "–Ω–µ–º–Ω–æ–≥–æ", "—á—É—Ç—å", "–µ–ª–µ"] and token.dep_ in ["advmod"]:
                intensifier = 0.5
            elif token.lemma_ in ["–≤—Ä—è–¥ –ª–∏", "–µ–¥–≤–∞ –ª–∏"] and token.dep_ in ["advmod"]:
                negation_count += 1
        sentiment_modifier = -intensifier if negation_count % 2 == 1 else intensifier
        if "–Ω–µ –æ—á–µ–Ω—å" in sentence.lower():
            sentiment_modifier *= 0.3
        logger.debug(f"–ú–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è '{self.log_safe(sentence)}': {sentiment_modifier}, negation_count={negation_count}")
        return sentiment_modifier

    def analyze_sentiment_transformers(self, texts: List[str]) -> List[Tuple[str, float]]:
        if not texts:
            return [("–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ", 0.0)]
        cached_results = [self.sentiment_cache.get(text, None) for text in texts]
        to_process = [text for i, text in enumerate(texts) if cached_results[i] is None]
        results = []
        if to_process and sentiment_analyzer:
            try:
                batch_results = sentiment_analyzer(to_process, batch_size=16)
                for text, result in zip(to_process, batch_results):
                    label = result['label'].lower()
                    score = result['score']
                    logger.info(f"–ú–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É–ª–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞ '{self.log_safe(text)}': label={label}, score={score}")
                    doc = nlp(text)
                    has_negative = any(token.lemma_ in [k for k, (s, _) in SENTIMENT_DICT.items() if s == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ"] for token in doc)
                    has_positive = any(token.lemma_ in [k for k, (s, _) in SENTIMENT_DICT.items() if s == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ"] for token in doc)
                    negative_boost = 0
                    if has_negative:
                        negative_words = [token.lemma_ for token in doc if SENTIMENT_DICT.get(token.lemma_, (None, None))[0] == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ"]
                        if negative_words:
                            negative_boost = -0.7 * len(negative_words) * min(abs(SENTIMENT_DICT[word][1]) for word in negative_words)
                        else:
                            negative_boost = 0
                    if label == "positive" and score > 0.6:
                        base_score = score
                    elif label == "negative" and score > 0.6:
                        base_score = -score
                    elif label == "neutral":
                        base_score = negative_boost if has_negative and negative_boost < -0.5 else 0.6 if has_positive else 0.0
                    else:
                        base_score = 0.0
                    modifier = self.check_modifiers_with_dependencies(text)
                    adjusted_score = base_score * modifier
                    if "!" in text:
                        adjusted_score *= 1.2
                    elif "..." in text or "?" in text:
                        adjusted_score *= 0.8
                    sentiment = "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ" if adjusted_score > 0.4 else "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ" if adjusted_score < -0.4 else "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ"
                    if sentiment != "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ" and abs(adjusted_score) > 0.8:
                        for token in doc:
                            if token.pos_ in ["ADJ", "ADV"] and token.lemma_ not in SENTIMENT_DICT:
                                self.update_sentiment_dict(token.lemma_, sentiment, adjusted_score)
                    if len(self.sentiment_cache) >= self.max_cache_size:
                        self.sentiment_cache.popitem(last=False)
                    self.sentiment_cache[text] = (sentiment, adjusted_score)
                    results.append((sentiment, adjusted_score))
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏: {e}. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å–ª–æ–≤–∞—Ä–Ω—ã–π –º–µ—Ç–æ–¥.")
                for text in to_process:
                    result = self.fallback_sentiment_analysis(text)
                    if len(self.sentiment_cache) >= self.max_cache_size:
                        self.sentiment_cache.popitem(last=False)
                    self.sentiment_cache[text] = result
                    results.append(result)
        final_results = []
        result_idx = 0
        for i, text in enumerate(texts):
            if cached_results[i] is not None:
                final_results.append(cached_results[i])
            else:
                final_results.append(results[result_idx])
                result_idx += 1
        return final_results

    def fallback_sentiment_analysis(self, text: str) -> Tuple[str, float]:
        doc = nlp(text.lower())
        sentiment_scores = {"–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ": 0, "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ": 0}
        for token in doc:
            sentiment, score = SENTIMENT_DICT.get(token.lemma_, ("–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ", 0.0))
            if sentiment in ["–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ"]:
                sentiment_scores[sentiment] += score
        modifier = self.check_modifiers_with_dependencies(text)
        total_score = (sentiment_scores["–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ"] + sentiment_scores["–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ"]) * modifier
        return ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", total_score) if total_score > 0.4 else ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", total_score) if total_score < -0.4 else ("–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ", 0.0)

    def extract_aspects(self, sentence: str) -> List[Tuple[str, str, float, str]]:
        doc = nlp(sentence)
        aspects = []
        invalid_words = {"–æ—á–µ–Ω–∫", "–ø–æ—Ä–æ–±–æ–≤–∞—Ç—å", "—Å–ø–∞—Å–Ω–æ–≥–æ", "–∑–∞—Å–∫—É—á–∞—Ç—å—Å—è", "–¥–æ–±–æ–≤—Å—Ç–≤–æ", "—Ö–∞—Ä–∏"}
        MAX_ASPECT_LENGTH = 3  # –û–≥—Ä–∞–Ω–∏—á–∏–º –¥–ª–∏–Ω—É –∞—Å–ø–µ–∫—Ç–∞ –¥–æ 3 —Å–ª–æ–≤

        if sentence.lower().strip() in ["–Ω–µ—Ç", "–Ω–µ—Ç—É"]:
            sentiment, score = self.get_sentiment("–Ω–µ—Ç")
            aspects.append(("–Ω–µ—Ç", sentiment, score, sentence))
            logger.info(f"–ò–∑–≤–ª–µ—á—ë–Ω –∞—Å–ø–µ–∫—Ç '–Ω–µ—Ç': —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å={sentiment}, —Å–∫–æ—Ä={score}")
            return aspects

        sentiment_results = self.analyze_sentiment_transformers([sentence])[0]
        sentiment, score = sentiment_results
        clauses = self.split_mixed_sentence(sentence)

        for clause in clauses:
            doc_clause = nlp(clause)
            for token in doc_clause:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∞—Å–ø–µ–∫—Ç—ã —Ç–æ–ª—å–∫–æ –¥–ª—è —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö, –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã—Ö –∏ –≥–ª–∞–≥–æ–ª–æ–≤
                if (token.pos_ in ["NOUN", "ADJ", "VERB"] and 
                    token.lemma_ not in invalid_words and 
                    (token.lemma_ in self.domain_hints or token.lemma_ in [k for k, _ in SENTIMENT_DICT.items()])):
                    modifiers = []
                    negation = False
                    children_count = 0
                    for child in token.children:
                        if (child.dep_ in ["amod", "compound", "advmod", "nmod", "obj"] and 
                            child.lemma_ not in invalid_words and 
                            children_count < MAX_ASPECT_LENGTH - 1):
                            modifiers.append(child.lemma_)
                            children_count += 1
                        if child.lemma_ in ["–Ω–µ", "–Ω–µ—Ç", "–Ω–∏", "–µ–¥–≤–∞"] and child.dep_ in ["neg"]:
                            negation = True
                    if token.dep_ in ["amod", "nmod", "obj"]:
                        parent = token.head
                        if parent.pos_ in ["NOUN", "VERB"] and parent.lemma_ not in invalid_words:
                            modifiers.append(parent.lemma_)
                    
                    # –§–æ—Ä–º–∏—Ä—É–µ–º –∞—Å–ø–µ–∫—Ç
                    aspect_phrase = " ".join(sorted([mod for mod in modifiers if mod] + [token.lemma_])).strip()
                    
                    # –§–∏–ª—å—Ç—Ä—É–µ–º –∞—Å–ø–µ–∫—Ç—ã
                    if (not aspect_phrase or 
                        aspect_phrase.lower() in self.invalid_phrases or 
                        len(aspect_phrase.split()) > MAX_ASPECT_LENGTH or
                        aspect_phrase.lower() == "—Ä—ã–±–∞"):  # –ò—Å–∫–ª—é—á–∞–µ–º –∞—Å–ø–µ–∫—Ç "—Ä—ã–±–∞", —Ç–∞–∫ –∫–∞–∫ —ç—Ç–æ —Å–∞–º –ø—Ä–æ–¥—É–∫—Ç
                        continue
                    
                    aspect_sentiment = sentiment
                    aspect_score = score
                    if negation:
                        aspect_sentiment = "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ" if sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ" else "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ"
                        aspect_score = -score
                    
                    aspects.append((aspect_phrase, aspect_sentiment, aspect_score, sentence))
                    logger.debug(f"–ò–∑–≤–ª–µ—á—ë–Ω –∞—Å–ø–µ–∫—Ç '{aspect_phrase}' —Å —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é '{aspect_sentiment}' –∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: {sentence}")
        
        return aspects

    def analyze_review_sentences(self, review_text: str) -> List[Tuple[str, str, float, List[Tuple[str, str, float, str]]]]:
        review_text = self.preprocess_text(review_text)
        sentences = self.split_sentences(review_text)
        result = []
        sentiments = self.analyze_sentiment_transformers(sentences)
        for sentence, (sentiment, score) in zip(sentences, sentiments):
            try:
                aspects = self.extract_aspects(sentence)
                result.append((sentence, sentiment, score, aspects))
                logger.info(f"–ê–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è '{self.log_safe(sentence)}': —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å={sentiment}, —Å–∫–æ—Ä={score}, –∞—Å–ø–µ–∫—Ç—ã={aspects}")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è '{self.log_safe(sentence)}': {str(e)}")
                result.append((sentence, "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ", 0.0, []))
        return result

    def validate_csv(self, df: pd.DataFrame) -> bool:
        required_columns_set1 = {"–î–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤–∞", "–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏", "–û—Ü–µ–Ω–∫–∞"}
        required_columns_set2 = {"–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞", "–û—Ü–µ–Ω–∫–∞"}
        columns = set(df.columns)
        if not (required_columns_set1.issubset(columns) or required_columns_set2.issubset(columns)):
            logger.error(f"CSV-—Ñ–∞–π–ª –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤. –û–∂–∏–¥–∞—é—Ç—Å—è: {required_columns_set1} –∏–ª–∏ {required_columns_set2}, –Ω–∞–π–¥–µ–Ω—ã: {columns}")
            raise ValueError("CSV-—Ñ–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –ª–∏–±–æ —Å—Ç–æ–ª–±—Ü—ã '–î–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤–∞', '–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏' –∏ '–û—Ü–µ–Ω–∫–∞', –ª–∏–±–æ '–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞' –∏ '–û—Ü–µ–Ω–∫–∞'")
        
        if "–û—Ü–µ–Ω–∫–∞" in df.columns:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ (0) –≤ 1
            invalid_zero_ratings = df["–û—Ü–µ–Ω–∫–∞"].apply(lambda x: isinstance(x, (int, float)) and x == 0)
            if invalid_zero_ratings.any():
                logger.warning(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –æ—Ü–µ–Ω–∫–∏ 0 –≤ —Å—Ç—Ä–æ–∫–∞—Ö: {df[invalid_zero_ratings].index.tolist()}. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏—Ö –≤ 1.")
                df.loc[invalid_zero_ratings, "–û—Ü–µ–Ω–∫–∞"] = 1
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ –æ—Ü–µ–Ω–∫–∏ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ –æ—Ç 1 –¥–æ 5
            invalid_ratings = df["–û—Ü–µ–Ω–∫–∞"].apply(lambda x: not (isinstance(x, (int, float)) and 1 <= x <= 5))
            if invalid_ratings.any():
                logger.error(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Å—Ç–æ–ª–±—Ü–µ '–û—Ü–µ–Ω–∫–∞' –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è: {df[invalid_ratings]['–û—Ü–µ–Ω–∫–∞'].tolist()}")
                raise ValueError("–í—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Å—Ç–æ–ª–±—Ü–µ '–û—Ü–µ–Ω–∫–∞' –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —á–∏—Å–ª–∞–º–∏ –æ—Ç 1 –¥–æ 5")
        return True

    def normalize_aspect(self, aspect: str) -> str:
        words = sorted(set(aspect.lower().split()))
        # –ü—Ä–∏–≤–æ–¥–∏–º "–≤–∫—É—Å–Ω—ã–π" –∫ "–≤–∫—É—Å" –∏ "–∫—Ä–µ–≤–µ—Ç–∫–∞ —Ä–∞–∑–º–µ—Ä" –∫ "—Ä–∞–∑–º–µ—Ä"
        normalized_words = []
        for word in words:
            if word == "–≤–∫—É—Å–Ω—ã–π":
                normalized_words.append("–≤–∫—É—Å")
            elif word == "–∫—Ä–µ–≤–µ—Ç–∫–∞" and "—Ä–∞–∑–º–µ—Ä" in words:
                continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º "–∫—Ä–µ–≤–µ—Ç–∫–∞", –µ—Å–ª–∏ –µ—Å—Ç—å "—Ä–∞–∑–º–µ—Ä"
            #elif word == product_name.lower():  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–∞
                continue
            else:
                normalized_words.append(word)
        
        # –ï—Å–ª–∏ –∞—Å–ø–µ–∫—Ç —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
        aspect_str = " ".join(normalized_words)
        for existing_aspect in self.positive_keywords | self.negative_keywords:
            existing_words = set(existing_aspect.lower().split())
            if existing_words.issubset(set(normalized_words)) and len(existing_words) <= len(normalized_words):
                return existing_aspect
        return aspect_str

    def analyze_reviews(self, csv_path: str) -> Dict[str, Union[str, Counter]]:
        logger.debug(f"–ù–∞—á–∞–ª–æ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∞–π–ª–∞: {csv_path}")
        try:
            # –ü–æ–ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏ –¥–ª—è —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞
            encodings = ['utf-8', 'cp1251', 'latin1']
            df = None
            for encoding in encodings:
                try:
                    df = pd.read_csv(csv_path, encoding=encoding)
                    logger.debug(f"–§–∞–π–ª {csv_path} —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ—á–∏—Ç–∞–Ω —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π {encoding}")
                    break
                except UnicodeDecodeError:
                    logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª {csv_path} —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π {encoding}, –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é")
                    continue
            if df is None:
                logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª {csv_path} —Å –¥–æ—Å—Ç—É–ø–Ω—ã–º–∏ –∫–æ–¥–∏—Ä–æ–≤–∫–∞–º–∏: {encodings}")
                return {
                    "–ü–ª—é—Å—ã": "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏",
                    "–ú–∏–Ω—É—Å—ã": "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏",
                    "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ)": "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏",
                    "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ)": "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏",
                    "–û–±—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞": "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏",
                    "–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏",
                    "–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏",
                    "–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏",
                    "positive_aspects": Counter(),
                    "negative_aspects": Counter()
                }

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç–æ–π —Ñ–∞–π–ª
            if df.empty:
                logger.warning(f"CSV-—Ñ–∞–π–ª {csv_path} –ø—É—Å—Ç–æ–π")
                return {
                    "–ü–ª—é—Å—ã": "–§–∞–π–ª –ø—É—Å—Ç",
                    "–ú–∏–Ω—É—Å—ã": "–§–∞–π–ª –ø—É—Å—Ç",
                    "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ)": "",
                    "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ)": "",
                    "–û–±—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞": "",
                    "–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": "0",
                    "–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": "0",
                    "–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": "0",
                    "positive_aspects": Counter(),
                    "negative_aspects": Counter()
                }

            # –í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã CSV
            self.validate_csv(df)
            logger.debug(f"–°—Ç—Ä—É–∫—Ç—É—Ä–∞ CSV-—Ñ–∞–π–ª–∞ {csv_path} –≤–∞–ª–∏–¥–Ω–∞")

            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—á—ë—Ç—á–∏–∫–æ–≤
            positive_aspects = Counter()
            negative_aspects = Counter()
            positive_count = negative_count = neutral_count = 0
            processed_texts_set = set()

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            self.review_data = []
            for _, row in df.iterrows():
                pros = row.get("–î–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤–∞", row.get("–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞", ""))
                cons = row.get("–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏", "")
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ NaN –≤ —Å—Ç–æ–ª–±—Ü–µ "–û—Ü–µ–Ω–∫–∞"
                rating = row["–û—Ü–µ–Ω–∫–∞"]
                if pd.isna(rating):
                    logger.warning(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ NaN –∑–Ω–∞—á–µ–Ω–∏–µ –≤ —Å—Ç–æ–ª–±—Ü–µ '–û—Ü–µ–Ω–∫–∞' –≤ —Å—Ç—Ä–æ–∫–µ {row.name}. –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ä–µ–π—Ç–∏–Ω–≥ 3.")
                    rating = 3.0
                else:
                    rating = float(rating)
                date = row.get("–î–∞—Ç–∞", "")
                username = row.get("–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å", "–ê–Ω–æ–Ω–∏–º")
                self.review_data.append({
                    "original_pros": str(pros),
                    "original_cons": str(cons),
                    "rating": rating,
                    "date": date,
                    "username": username
                })

            # –ê–Ω–∞–ª–∏–∑ –æ—Ç–∑—ã–≤–æ–≤
            for _, row in df.iterrows():
                pros_text = str(row.get("–î–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤–∞", row.get("–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞", "")))
                cons_text = str(row.get("–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏", ""))
                rating = float(row["–û—Ü–µ–Ω–∫–∞"])  # –ü–æ—Å–ª–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∑–¥–µ—Å—å —É–∂–µ –Ω–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å NaN
                logger.debug(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–∑—ã–≤–∞: –î–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤–∞='{pros_text}', –ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏='{cons_text}', –û—Ü–µ–Ω–∫–∞={rating}")

                for text in [pros_text, cons_text]:
                    if text.strip():
                        processed_texts_set.add(text)
                        sentences = self.analyze_review_sentences(text)
                        for _, sentiment, _, aspects in sentences:
                            if aspects:
                                for aspect_phrase, aspect_sentiment, _, _ in aspects:
                                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∞—Å–ø–µ–∫—Ç –ø–µ—Ä–µ–¥ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º
                                    normalized_aspect = self.normalize_aspect(aspect_phrase)
                                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –∞—Å–ø–µ–∫—Ç–∞
                                    aspect_text_sentiment, _ = self.analyze_sentiment_transformers([normalized_aspect])[0]
                                    if aspect_text_sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ":
                                        negative_aspects[normalized_aspect] += 1
                                        self.negative_keywords.add(normalized_aspect)
                                    elif aspect_sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ":
                                        positive_aspects[normalized_aspect] += 1
                                        self.positive_keywords.add(normalized_aspect)
                                    elif aspect_sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ":
                                        negative_aspects[normalized_aspect] += 1
                                        self.negative_keywords.add(normalized_aspect)

                if rating >= self.positive_threshold:
                    positive_count += 1
                elif rating <= self.negative_threshold:
                    negative_count += 1
                else:
                    neutral_count += 1

            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–µ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –∞—Å–ø–µ–∫—Ç–æ–≤
            for aspect in list(positive_aspects.keys()):
                if aspect.lower() in ["–Ω–µ—Ç –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–æ–≤", "–Ω–µ—Ç"]:
                    del positive_aspects[aspect]
                    self.positive_keywords.discard(aspect)
            for aspect in list(negative_aspects.keys()):
                if aspect.lower() in ["–Ω–µ—Ç", "–Ω–µ—Ç—É"]:
                    sentiment, _ = self.analyze_sentiment_transformers([aspect])[0]
                    if sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ":
                        positive_aspects[aspect] += negative_aspects[aspect]
                        self.positive_keywords.add(aspect)
                        del negative_aspects[aspect]
                        self.negative_keywords.discard(aspect)

            # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è –∞—Å–ø–µ–∫—Ç–æ–≤
            common_aspects = set(positive_aspects.keys()).intersection(set(negative_aspects.keys()))
            for aspect in common_aspects:
                sentiment, _ = self.analyze_sentiment_transformers([aspect])[0]
                if sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ":
                    del negative_aspects[aspect]
                    self.negative_keywords.discard(aspect)
                elif sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ":
                    del positive_aspects[aspect]
                    self.positive_keywords.discard(aspect)
                else:
                    pos_count = positive_aspects[aspect]
                    neg_count = negative_aspects[aspect]
                    if pos_count > neg_count:
                        del negative_aspects[aspect]
                        self.negative_keywords.discard(aspect)
                    else:
                        del positive_aspects[aspect]
                        self.positive_keywords.discard(aspect)

            main_positives = "\n".join([f"{aspect} ({count})" for aspect, count in positive_aspects.most_common(5)]) if positive_aspects else "–ù–µ—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤"
            main_negatives = "\n".join([f"{aspect} ({count})" for aspect, count in negative_aspects.most_common(5)]) if negative_aspects else "–ù–µ—Ç –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤"

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫—ç—à–∞ –ø–æ—Å–ª–µ –∞–Ω–∞–ª–∏–∑–∞
            self.save_cache()

            result = {
                "–ü–ª—é—Å—ã": main_positives,
                "–ú–∏–Ω—É—Å—ã": main_negatives,
                "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ)": ", ".join(sorted(self.positive_keywords)[:10]),
                "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ)": ", ".join(sorted(self.negative_keywords)[:10]),
                "–û–±—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞": ", ".join(sorted(set(self.positive_keywords).union(self.negative_keywords))[:15]),
                "–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": str(positive_count),
                "–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": str(negative_count),
                "–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": str(neutral_count),
                "positive_aspects": positive_aspects,
                "negative_aspects": negative_aspects
            }
            logger.debug(f"–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞: {result}")
            return result

        except FileNotFoundError as e:
            logger.error(f"–§–∞–π–ª {csv_path} –Ω–µ –Ω–∞–π–¥–µ–Ω: {str(e)}")
            return {
                "–ü–ª—é—Å—ã": "–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω",
                "–ú–∏–Ω—É—Å—ã": "–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω",
                "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ)": "–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω",
                "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ)": "–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω",
                "–û–±—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞": "–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω",
                "–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": "–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω",
                "–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": "–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω",
                "–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": "–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω",
                "positive_aspects": Counter(),
                "negative_aspects": Counter()
            }
        except UnicodeDecodeError as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ {csv_path}: {str(e)}")
            return {
                "–ü–ª—é—Å—ã": "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏",
                "–ú–∏–Ω—É—Å—ã": "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏",
                "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ)": "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏",
                "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ)": "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏",
                "–û–±—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞": "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏",
                "–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏",
                "–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏",
                "–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏",
                "positive_aspects": Counter(),
                "negative_aspects": Counter()
            }
        except ValueError as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ CSV-—Ñ–∞–π–ª–∞ {csv_path}: {str(e)}")
            return {
                "–ü–ª—é—Å—ã": f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {str(e)}",
                "–ú–∏–Ω—É—Å—ã": f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {str(e)}",
                "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ)": f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {str(e)}",
                "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ)": f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {str(e)}",
                "–û–±—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞": f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {str(e)}",
                "–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {str(e)}",
                "–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {str(e)}",
                "–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {str(e)}",
                "positive_aspects": Counter(),
                "negative_aspects": Counter()
            }
        except Exception as e:
            logger.error(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Ñ–∞–π–ª–∞ {csv_path}: {str(e)}")
            return {
                "–ü–ª—é—Å—ã": "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞",
                "–ú–∏–Ω—É—Å—ã": "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞",
                "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ)": "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞",
                "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ)": "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞",
                "–û–±—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞": "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞",
                "–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞",
                "–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞",
                "–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞",
                "positive_aspects": Counter(),
                "negative_aspects": Counter()
            }

    def aggregate_reviews(self, csv_paths: List[str]) -> Dict[str, Union[str, Counter]]:
        all_keywords: List[str] = []
        total_positive_count = total_negative_count = total_neutral_count = 0
        all_positive_aspects = Counter()
        all_negative_aspects = Counter()

        with Pool(processes=4) as pool:
            results = pool.map(self.analyze_reviews, csv_paths)

        for result in results:
            if result["–ü–ª—é—Å—ã"] not in ["–û—à–∏–±–∫–∞", "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏", "–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞"]:
                positive_count = int(result["–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã"])
                negative_count = int(result["–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã"])
                neutral_count = int(result["–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã"])
                total_positive_count += positive_count
                total_negative_count += negative_count
                total_neutral_count += neutral_count
                all_positive_aspects.update(result["positive_aspects"])
                all_negative_aspects.update(result["negative_aspects"])
                all_keywords.extend(result["–û–±—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞"].split(", "))

        main_positives = "\n".join([f"{aspect} ({count})" for aspect, count in all_positive_aspects.most_common(5)]) if all_positive_aspects else "–ù–µ—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤"
        main_negatives = "\n".join([f"{aspect} ({count})" for aspect, count in all_negative_aspects.most_common(5)]) if all_negative_aspects else "–ù–µ—Ç –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤"
        top_keywords = ", ".join(sorted(set(all_keywords))[:15]) if all_keywords else "–ù–µ—Ç –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"

        return {
            "–ü–ª—é—Å—ã (–≤—Å–µ —Å–∞–π—Ç—ã)": main_positives,
            "–ú–∏–Ω—É—Å—ã (–≤—Å–µ —Å–∞–π—Ç—ã)": main_negatives,
            "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ, –≤—Å–µ —Å–∞–π—Ç—ã)": ", ".join(sorted(self.positive_keywords)[:10]),
            "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ, –≤—Å–µ —Å–∞–π—Ç—ã)": ", ".join(sorted(self.negative_keywords)[:10]),
            "–û–±—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–≤—Å–µ —Å–∞–π—Ç—ã)": top_keywords,
            "–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã (–≤—Å–µ —Å–∞–π—Ç—ã)": str(total_positive_count),
            "–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã (–≤—Å–µ —Å–∞–π—Ç—ã)": str(total_negative_count),
            "–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã (–≤—Å–µ —Å–∞–π—Ç—ã)": str(total_neutral_count),
            "positive_aspects": all_positive_aspects,
            "negative_aspects": all_negative_aspects
        }

    def get_sentiment(self, word: str) -> Tuple[str, float]:
        return SENTIMENT_DICT.get(word, ("–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ", 0.0))

    def update_sentiment_dict(self, word: str, sentiment: str, score: float):
        if word in SENTIMENT_DICT:
            current_sentiment, current_score = SENTIMENT_DICT[word]
            new_score = (current_score + score) / 2
            SENTIMENT_DICT[word] = (sentiment, new_score)
        else:
            SENTIMENT_DICT[word] = (sentiment, score)
        logger.info(f"–û–±–Ω–æ–≤–ª—ë–Ω —Å–ª–æ–≤–∞—Ä—å —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏: {word} -> {SENTIMENT_DICT[word]}")

    def find_representative_examples(self, aspect: str, aspect_sentiment: str, max_examples: int = 2) -> List[str]:
        examples = []
        aspect_words = set(aspect.lower().split())
        positive_examples = []
        negative_examples = []
        seen_examples = set()  # –î–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤

        for review in self.review_data:
            pros_text = review['original_pros'].lower() if review['original_pros'] else ''
            cons_text = review['original_cons'].lower() if review['original_cons'] else ''
            review_text = pros_text + ' ' + cons_text

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≤—Å–µ —Å–ª–æ–≤–∞ –∞—Å–ø–µ–∫—Ç–∞ –≤ —Ç–µ–∫—Å—Ç–µ –æ—Ç–∑—ã–≤–∞
            if not all(word in review_text for word in aspect_words):
                continue

            sentiment, _ = self.analyze_sentiment_transformers([review_text])[0]
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
            if (aspect_sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ" and sentiment != "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ") or \
            (aspect_sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ" and sentiment != "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ"):
                continue

            example_text = review['original_pros'] if pros_text else review['original_cons']
            example = f'"{example_text}" ({review["username"]})'

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è –ø—Ä–∏–º–µ—Ä—ã
            if example in seen_examples:
                continue
            seen_examples.add(example)

            logger.debug(f"–ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Ç–∑—ã–≤ –¥–ª—è –∞—Å–ø–µ–∫—Ç–∞ '{aspect}': —Ç–µ–∫—Å—Ç='{example_text}', —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å={sentiment}")

            if sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ":
                positive_examples.append(example)
            elif sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ":
                negative_examples.append(example)

        if aspect_sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ":
            examples.extend(positive_examples[:max_examples])
        elif aspect_sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ":
            examples.extend(negative_examples[:max_examples])

        if not examples:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –∞—Å–ø–µ–∫—Ç–∞ '{aspect}' —Å —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é '{aspect_sentiment}'")
        else:
            logger.info(f"–ù–∞–π–¥–µ–Ω—ã –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –∞—Å–ø–µ–∫—Ç–∞ '{aspect}': {examples}")

        return examples[:max_examples]

    def analyze_trends(self) -> str:
        if not self.review_data or not any(review['date'] for review in self.review_data):
            logger.warning("–î–∞–Ω–Ω—ã–µ –æ –¥–∞—Ç–∞—Ö –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ review_data.")
            return "–î–∞–Ω–Ω—ã–µ –æ –¥–∞—Ç–∞—Ö –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç.\n"

        # –°–ª–æ–≤–∞—Ä—å –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ä—É—Å—Å–∫–∏—Ö –Ω–∞–∑–≤–∞–Ω–∏–π –º–µ—Å—è—Ü–µ–≤
        month_mapping = {
            "—è–Ω–≤": "01", "—è–Ω–≤–∞—Ä—è": "01",
            "—Ñ–µ–≤": "02", "—Ñ–µ–≤—Ä–∞–ª—è": "02",
            "–º–∞—Ä": "03", "–º–∞—Ä—Ç–∞": "03",
            "–∞–ø—Ä": "04", "–∞–ø—Ä–µ–ª—è": "04",
            "–º–∞–π": "05", "–º–∞—è": "05",
            "–∏—é–Ω": "06", "–∏—é–Ω—è": "06",
            "–∏—é–ª": "07", "–∏—é–ª—è": "07",
            "–∞–≤–≥": "08", "–∞–≤–≥—É—Å—Ç–∞": "08",
            "—Å–µ–Ω": "09", "—Å–µ–Ω—Ç—è–±—Ä—è": "09",
            "–æ–∫—Ç": "10", "–æ–∫—Ç—è–±—Ä—è": "10",
            "–Ω–æ—è": "11", "–Ω–æ—è–±—Ä—è": "11",
            "–¥–µ–∫": "12", "–¥–µ–∫–∞–±—Ä—è": "12"
        }

        reviews_with_dates = []
        date_formats = [
            "%d.%m.%Y",  # 27.04.2024
            "%Y-%m-%d",  # 2023-10-15
            "%d/%m/%Y",  # 15/10/2023
            "%Y/%m/%d",  # 2023/10/15
            "%d-%m-%Y",  # 15-10-2023
        ]

        for review in self.review_data:
            if not review['date']:
                continue

            date_str = str(review['date']).strip()
            date = None

            for date_format in date_formats:
                try:
                    date = pd.to_datetime(date_str, format=date_format, errors='coerce', dayfirst=True)
                    if pd.notna(date):
                        break
                except Exception:
                    continue

            if pd.isna(date):
                parts = date_str.split()
                if len(parts) == 3:
                    day, month_str, year = parts
                    month_str = month_str.lower()
                    if month_str in month_mapping:
                        month = month_mapping[month_str]
                        day = day.zfill(2)
                        normalized_date_str = f"{day}.{month}.{year}"
                        try:
                            date = pd.to_datetime(normalized_date_str, format="%d.%m.%Y", errors='coerce')
                        except Exception as e:
                            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é –¥–∞—Ç—É '{normalized_date_str}': {str(e)}")

            if pd.notna(date):
                reviews_with_dates.append((date, review['rating']))
            else:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –¥–∞—Ç—É '{date_str}' –≤ —Å—Ç—Ä–æ–∫–µ {review}. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")

        if not reviews_with_dates:
            logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –Ω–∏ –æ–¥–Ω—É –¥–∞—Ç—É –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–Ω–¥–æ–≤.")
            return "–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –¥–∞—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.\n"

        df = pd.DataFrame(reviews_with_dates, columns=['date', 'rating'])
        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –ø–æ –≥–æ–¥–∞–º –≤–º–µ—Å—Ç–æ –º–µ—Å—è—Ü–µ–≤
        df['year'] = df['date'].dt.to_period('Y')
        yearly_stats = df.groupby('year')['rating'].agg(['mean', 'count']).reset_index()
        yearly_stats['year'] = yearly_stats['year'].astype(str)

        trend_report = "–¢—Ä–µ–Ω–¥—ã –ø–æ –≥–æ–¥–∞–º:\n"
        trend_report += "-" * 30 + "\n"
        for _, row in yearly_stats.iterrows():
            trend_report += f"{row['year']}: –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ {row['mean']:.1f} (–æ—Ç–∑—ã–≤–æ–≤: {row['count']})\n"

        if len(yearly_stats) >= 2:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—â–∏–π —Ç—Ä–µ–Ω–¥ —Å –ø–æ–º–æ—â—å—é –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
            yearly_stats['index'] = range(len(yearly_stats))
            slope, _ = np.polyfit(yearly_stats['index'], yearly_stats['mean'], 1)
            if slope > 0.1:
                trend_report += f"\n–ù–∞–±–ª—é–¥–∞–µ—Ç—Å—è —É–ª—É—á—à–µ–Ω–∏–µ: —Å—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ –≤—ã—Ä–æ—Å–ª–∞ —Å {yearly_stats['mean'].iloc[0]:.1f} –¥–æ {yearly_stats['mean'].iloc[-1]:.1f}.\n"
            elif slope < -0.1:
                trend_report += f"\n–ù–∞–±–ª—é–¥–∞–µ—Ç—Å—è —É—Ö—É–¥—à–µ–Ω–∏–µ: —Å—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ —Å–Ω–∏–∑–∏–ª–∞—Å—å —Å {yearly_stats['mean'].iloc[0]:.1f} –¥–æ {yearly_stats['mean'].iloc[-1]:.1f}.\n"
            else:
                trend_report += "\n–û—Ü–µ–Ω–∫–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Å—Ç–∞–±–∏–ª—å–Ω—ã.\n"

        return trend_report + "\n"

    def generate_detailed_report(self, analysis_result: Dict[str, Union[str, Counter]], product_name: str = "–ø—Ä–æ–¥—É–∫—Ç", site: str = "–ù–µ —É–∫–∞–∑–∞–Ω") -> str:
        positive_aspects = analysis_result["positive_aspects"]
        negative_aspects = analysis_result["negative_aspects"]
        positive_count = int(analysis_result["–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã"])
        negative_count = int(analysis_result["–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã"])
        neutral_count = int(analysis_result["–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã"])
        total_reviews = positive_count + negative_count + neutral_count

        # –ù–∞—á–∞–ª–æ –æ—Ç—á—ë—Ç–∞
        current_time = datetime.now().strftime("%d.%m.%Y %H:%M")
        report = ""
        if site != "–ù–µ —É–∫–∞–∑–∞–Ω":
            report += f"–û—Ç—á—ë—Ç –¥–ª—è {site}:\n\n"
        report += "–û—Ç—á—ë—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –æ—Ç–∑—ã–≤–æ–≤\n"
        report += "–ê–≤—Ç–æ—Ä: –ù–∏–∫–∏—Ç–∞ –ß–µ–ª—ã—à–µ–≤\n"
        report += f"–î–∞—Ç–∞: {current_time}\n"
        report += f"–ü—Ä–æ–¥—É–∫—Ç: {product_name}\n"
        report += "=" * 50 + "\n\n"

        # 1. –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞
        report += "1. –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞\n"
        report += "-" * 30 + "\n"
        if not positive_aspects:
            report += "–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç.\n\n"
        else:
            for aspect, count in positive_aspects.most_common(5):
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã
                if len(aspect.split()) > 3 or aspect.lower() in ["–Ω–µ—Ç", "–Ω–µ—Ç—É"]:
                    continue
                examples = self.find_representative_examples(aspect, "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ")
                # –ò—Å–∫–ª—é—á–∞–µ–º –Ω–µ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
                if examples and all("–Ω–µ—Ç" in ex.lower() for ex in examples):
                    examples = []
                examples_str = ", ".join(examples) if examples else "–ü—Ä–∏–º–µ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã."
                formatted_aspect = ' '.join(word.capitalize() for word in aspect.split())
                report += f"- {formatted_aspect} ({count} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π)\n"
                report += f"  –ü—Ä–∏–º–µ—Ä—ã: {examples_str}\n\n"

        # 2. –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏
        report += "2. –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏\n"
        report += "-" * 30 + "\n"
        if not negative_aspects:
            report += "–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç.\n\n"
        else:
            for aspect, count in negative_aspects.most_common(5):
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã
                if len(aspect.split()) > 3 or aspect.lower() in ["–Ω–µ—Ç", "–Ω–µ—Ç—É"]:
                    continue
                examples = self.find_representative_examples(aspect, "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ")
                # –ò—Å–∫–ª—é—á–∞–µ–º –Ω–µ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
                if examples and all("–Ω–µ—Ç" in ex.lower() for ex in examples):
                    examples = []
                examples_str = ", ".join(examples) if examples else "–ü—Ä–∏–º–µ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã."
                formatted_aspect = ' '.join(word.capitalize() for word in aspect.split())
                report += f"- {formatted_aspect} ({count} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π)\n"
                report += f"  –ü—Ä–∏–º–µ—Ä—ã: {examples_str}\n\n"

        # 3. –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        report += "3. –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞\n"
        report += "-" * 30 + "\n"
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        positive_keywords = []
        negative_keywords = []
        for kw in sorted(self.positive_keywords):
            if len(kw.split()) <= 3:
                kw_sentiment, _ = self.analyze_sentiment_transformers([kw])[0]
                if kw_sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ":
                    positive_keywords.append(kw)
        positive_keywords = positive_keywords[:10]

        for kw in sorted(self.negative_keywords):
            if len(kw.split()) <= 3:
                kw_sentiment, _ = self.analyze_sentiment_transformers([kw])[0]
                if kw_sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ":
                    negative_keywords.append(kw)
        negative_keywords = negative_keywords[:10]

        common_keywords = sorted(set(positive_keywords).union(negative_keywords))[:15]
        report += f"–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ: {', '.join(positive_keywords) if positive_keywords else '–ù–µ—Ç'}\n"
        report += f"–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ: {', '.join(negative_keywords) if negative_keywords else '–ù–µ—Ç'}\n"
        report += f"–û–±—â–∏–µ: {', '.join(common_keywords) if common_keywords else '–ù–µ—Ç'}\n\n"

        # 4. –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        report += "4. –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n"
        report += "-" * 30 + "\n"
        positive_percentage = (positive_count / total_reviews * 100) if total_reviews > 0 else 0
        negative_percentage = (negative_count / total_reviews * 100) if total_reviews > 0 else 0
        neutral_percentage = (neutral_count / total_reviews * 100) if total_reviews > 0 else 0
        average_rating = sum(review['rating'] for review in self.review_data) / total_reviews if total_reviews > 0 else 0
        report += f"–í—Å–µ–≥–æ –æ—Ç–∑—ã–≤–æ–≤: {total_reviews}\n"
        report += f"–°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞: {average_rating:.1f}/5\n"
        report += f"–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã (4-5): {positive_count} ({positive_percentage:.1f}%)\n"
        report += f"–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã (1-2): {negative_count} ({negative_percentage:.1f}%)\n"
        report += f"–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã (3): {neutral_count} ({neutral_percentage:.1f}%)\n\n"

        # 5. –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤
        report += "5. –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤\n"
        report += "-" * 30 + "\n"
        trend_analysis = self.analyze_trends()
        report += trend_analysis

        # 6. –û–±—â–µ–µ –≤–ø–µ—á–∞—Ç–ª–µ–Ω–∏–µ
        report += "6. –û–±—â–µ–µ –≤–ø–µ—á–∞—Ç–ª–µ–Ω–∏–µ\n"
        report += "-" * 30 + "\n"
        top_positive_aspects = [aspect for aspect, _ in positive_aspects.most_common(3) if len(aspect.split()) <= 3 and aspect.lower() != product_name.lower()]
        top_negative_aspects = [aspect for aspect, _ in negative_aspects.most_common(3) if len(aspect.split()) <= 3 and aspect.lower() != product_name.lower()]
        if positive_percentage > negative_percentage:
            impression = f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –≤ —Ü–µ–ª–æ–º –¥–æ–≤–æ–ª—å–Ω—ã –ø—Ä–æ–¥—É–∫—Ç–æ–º '{product_name}'.\n"
            impression += f"–û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞: {', '.join(top_positive_aspects).lower() if top_positive_aspects else '–Ω–µ —É–∫–∞–∑–∞–Ω—ã'}.\n"
            impression += f"–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã —Å–æ—Å—Ç–∞–≤–ª—è—é—Ç {positive_percentage:.1f}%.\n"
            if top_negative_aspects:
                impression += f"–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏: {', '.join(top_negative_aspects).lower()} ({negative_percentage:.1f}% –æ—Ç–∑—ã–≤–æ–≤).\n"
        else:
            impression = f"–í–ø–µ—á–∞—Ç–ª–µ–Ω–∏—è –æ –ø—Ä–æ–¥—É–∫—Ç–µ '{product_name}' —Å–º–µ—à–∞–Ω–Ω—ã–µ.\n"
            impression += f"–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞: {', '.join(top_positive_aspects).lower() if top_positive_aspects else '–Ω–µ —É–∫–∞–∑–∞–Ω—ã'} ({positive_percentage:.1f}% –æ—Ç–∑—ã–≤–æ–≤).\n"
            impression += f"–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏: {', '.join(top_negative_aspects).lower() if top_negative_aspects else '–Ω–µ —É–∫–∞–∑–∞–Ω—ã'} ({negative_percentage:.1f}% –æ—Ç–∑—ã–≤–æ–≤).\n"
        report += impression + "\n"

        # 7. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        report += "7. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n"
        report += "-" * 30 + "\n"
        report += f"–°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ –ø—Ä–æ–¥—É–∫—Ç–∞: {average_rating:.1f}/5.\n"
        if average_rating >= 4.0:
            report += "–ü—Ä–æ–¥—É–∫—Ç –ø–æ–ª—É—á–∞–µ—Ç –≤—ã—Å–æ–∫—É—é –æ—Ü–µ–Ω–∫—É.\n"
        elif average_rating >= 3.0:
            report += "–ü—Ä–æ–¥—É–∫—Ç –≤–æ—Å–ø—Ä–∏–Ω–∏–º–∞–µ—Ç—Å—è —É–º–µ—Ä–µ–Ω–Ω–æ.\n"
        else:
            report += "–ü—Ä–æ–¥—É–∫—Ç –∏–º–µ–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã.\n"
        if negative_aspects:
            top_negatives = [aspect for aspect, _ in negative_aspects.most_common(2) if len(aspect.split()) <= 3 and aspect.lower() != product_name.lower()]
            if top_negatives:
                report += f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–ª—É—á—à–∏—Ç—å: {', '.join(top_negatives).lower()}.\n"
                # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                negative_text = " ".join(top_negatives).lower()
                if "—Ç—É—Ö–ª—ã–π" in negative_text or "–ø—Ä–æ–≥–æ—Ä–∫–ª—ã–π" in negative_text or "–≤–æ–Ω—é—á–∏–π" in negative_text:
                    report += "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –æ–±—Ä–∞—Ç–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –∫–æ–Ω—Ç—Ä–æ–ª—å –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Å–≤–µ–∂–µ—Å—Ç–∏ –ø—Ä–æ–¥—É–∫—Ç–∞.\n"
                elif "—Ä–∞–∑–º–µ—Ä" in negative_text or "–º–µ–ª–∫–∏–π" in negative_text:
                    report += "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: —É–≤–µ–ª–∏—á–∏—Ç—å —Ä–∞–∑–º–µ—Ä –∫—Ä–µ–≤–µ—Ç–æ–∫ –∏–ª–∏ —É—Ç–æ—á–Ω–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–∞–∑–º–µ—Ä–µ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏.\n"
                elif "—Ü–µ–Ω–∞" in negative_text or "–¥–æ—Ä–æ–≥–æ–π" in negative_text:
                    report += "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –ø–µ—Ä–µ—Å–º–æ—Ç—Ä–µ—Ç—å —Ü–µ–Ω–æ–≤—É—é –ø–æ–ª–∏—Ç–∏–∫—É –∏–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –∞–∫—Ü–∏–∏.\n"
                elif "—É–ø–∞–∫–æ–≤–∫–∞" in negative_text:
                    report += "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: —É–ª—É—á—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ —É–ø–∞–∫–æ–≤–∫–∏ –∏–ª–∏ –µ—ë —É–¥–æ–±—Å—Ç–≤–æ.\n"
                else:
                    report += "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –ø—Ä–æ–≤–µ—Å—Ç–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –ø—Ä–∏—á–∏–Ω –Ω–µ–¥–æ–≤–æ–ª—å—Å—Ç–≤–∞.\n"
            else:
                report += "–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Å—Ç–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º.\n"
        else:
            report += "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —É–ª—É—á—à–µ–Ω–∏—é –Ω–µ—Ç.\n"

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ –≤ JSON
        result_data = {
            "positive_aspects": dict(positive_aspects),
            "negative_aspects": dict(negative_aspects),
            "positive_count": positive_count,
            "negative_count": negative_count,
            "neutral_count": neutral_count,
            "average_rating": average_rating,
            "positive_keywords": list(self.positive_keywords),
            "negative_keywords": list(self.negative_keywords)
        }
        with open("analysis_results.json", "w", encoding="utf-8") as f:
            json.dump(result_data, f, ensure_ascii=False, indent=4)

        return report

# –Æ–Ω–∏—Ç-—Ç–µ—Å—Ç—ã
if __name__ == "__main__":
    import unittest

    class TestReviewAnalyzer(unittest.TestCase):
        def setUp(self):
            self.analyzer = ReviewAnalyzer(use_preprocessing=False)

        def test_preprocess_text(self):
            text = "–•–æ—Ä–æ—à–∏–π –ø—Ä–æ–¥—É–∫—Ç üëç"
            expected = "—Ö–æ—Ä–æ—à–∏–π –ø—Ä–æ–¥—É–∫—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π_—ç–º–æ–¥–∑–∏"
            result = self.analyzer.preprocess_text(text)
            self.assertEqual(result, expected)

        def test_analyze_sentiment_transformers(self):
            texts = ["–û—Ç–ª–∏—á–Ω—ã–π –ø—Ä–æ–¥—É–∫—Ç!", "–£–∂–∞—Å–Ω—ã–π —Å–µ—Ä–≤–∏—Å"]
            results = self.analyzer.analyze_sentiment_transformers(texts)
            self.assertEqual(len(results), 2)
            self.assertEqual(results[0][0], "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ")
            self.assertEqual(results[1][0], "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ")

        def test_split_mixed_sentence(self):
            sentence = "–≠–∫—Ä–∞–Ω —è—Ä–∫–∏–π, –Ω–æ –±—ã—Å—Ç—Ä–æ —Ä–∞–∑—Ä—è–∂–∞–µ—Ç—Å—è"
            expected = ["—ç–∫—Ä–∞–Ω —è—Ä–∫–∏–π", "–±—ã—Å—Ç—Ä–æ —Ä–∞–∑—Ä—è–∂–∞–µ—Ç—Å—è"]
            result = self.analyzer.split_mixed_sentence(sentence)
            self.assertEqual(result, expected)

        def test_validate_csv(self):
            data = pd.DataFrame({
                "–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞": ["–•–æ—Ä–æ—à–∏–π –ø—Ä–æ–¥—É–∫—Ç", "–ü–ª–æ—Ö–æ–π —Å–µ—Ä–≤–∏—Å"],
                "–û—Ü–µ–Ω–∫–∞": [5, 1]
            })
            self.assertTrue(self.analyzer.validate_csv(data))
            invalid_data = pd.DataFrame({
                "–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞": ["–•–æ—Ä–æ—à–∏–π –ø—Ä–æ–¥—É–∫—Ç"],
                "–û—Ü–µ–Ω–∫–∞": [6]
            })
            with self.assertRaises(ValueError):
                self.analyzer.validate_csv(invalid_data)

        def test_extract_aspects(self):
            sentence = "–≠–∫—Ä–∞–Ω —è—Ä–∫–∏–π, –Ω–æ –±–∞—Ç–∞—Ä–µ—è —Å–ª–∞–±–∞—è"
            aspects = self.analyzer.extract_aspects(sentence)
            self.assertTrue(len(aspects) > 0)
            self.assertIn(aspects[0][1], ["–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ"])

        def test_analyze_review_sentences(self):
            review_text = "–ü—Ä–æ–¥—É–∫—Ç –æ—Ç–ª–∏—á–Ω—ã–π, –Ω–æ –¥–æ—Å—Ç–∞–≤–∫–∞ –º–µ–¥–ª–µ–Ω–Ω–∞—è."
            result = self.analyzer.analyze_review_sentences(review_text)
            self.assertTrue(len(result) > 0)
            self.assertIn(result[0][1], ["–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ"])

    unittest.main()