import pandas as pd
import spacy
from collections import Counter
import logging
from typing import List, Dict, Set, Union
import re
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
import pymorphy3
from concurrent.futures import ThreadPoolExecutor
import numpy as np

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ YandexSpeller —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–∫–∏
try:
    from pyaspeller import YandexSpeller
    speller = YandexSpeller()
except ImportError:
    logging.warning("–ú–æ–¥—É–ª—å pyaspeller –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –±—É–¥–µ—Ç –æ—Ç–∫–ª—é—á–µ–Ω–∞.")
    speller = None

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = logging.getLogger(__name__)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è spaCy –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ —Å –æ—Ç–∫–ª—é—á–µ–Ω–∏–µ–º –Ω–µ–Ω—É–∂–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
nlp = spacy.load("ru_core_news_sm", disable=["ner", "lemmatizer"])

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è pymorphy3 –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–ª–æ–≤
morph = pymorphy3.MorphAnalyzer()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
model_name = "cointegrated/rubert-tiny-sentiment-balanced"
sentiment_analyzer = None
try:
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name)
    sentiment_analyzer = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
except Exception as e:
    logger.warning(f"Error loading model {model_name}: {e}. Falling back to dictionary-based sentiment analysis.")

# –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
SENTIMENT_DICT = {
    "—Ö–æ—Ä–æ—à–∏–π": "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ",
    "–æ—Ç–ª–∏—á–Ω—ã–π": "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ",
    "–ø—Ä–µ–∫—Ä–∞—Å–Ω—ã–π": "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ",
    "—É–¥–æ–±–Ω—ã–π": "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ",
    "–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π": "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ",
    "–±—ã—Å—Ç—Ä—ã–π": "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ",
    "–Ω–∞–¥–µ–∂–Ω—ã–π": "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ",
    "–≤–∫—É—Å–Ω—ã–π": "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ",
    "–∫—Ä–∞—Å–∏–≤—ã–π": "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ",
    "–ø—Ä–∏—è—Ç–Ω—ã–π": "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ",
    "–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω—ã–π": "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ",
    "—Å–≤–µ–∂–∏–π": "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ",
    "–ø–ª–æ—Ö–æ–π": "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ",
    "—É–∂–∞—Å–Ω—ã–π": "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ",
    "–¥–µ—Ñ–µ–∫—Ç–Ω—ã–π": "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ",
    "–Ω–µ—É–¥–æ–±–Ω—ã–π": "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ",
    "–º–µ–¥–ª–µ–Ω–Ω—ã–π": "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ",
    "–Ω–µ–Ω–∞–¥–µ–∂–Ω—ã–π": "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ",
    "–Ω–µ–≤–∫—É—Å–Ω—ã–π": "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ",
    "—Å–ª–æ–º–∞–Ω–Ω—ã–π": "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ",
    "–≥—Ä—è–∑–Ω—ã–π": "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ",
    "–¥–æ—Ä–æ–≥–æ–π": "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ",
    "–±–µ–∑–≤–∫—É—Å–Ω—ã–π": "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ",
    "–ø—Ä–æ–≥–æ—Ä–∫–ª—ã–π": "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ",
    "–ø—Ä–æ—Ç—É—Ö—à–∏–π": "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ",
    "—Ç—Ä—ã–Ω–¥–µ—Ü": "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ",
    "—Ö—É–¥—à–∏–π": "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ",
    "–≤—ã—Å–æ–∫–∏–π": "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ",
    "–ø—Ä–æ—Å—Ç–æ": "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ",
    "—Ä–∞–∑–≤–∏–≤–∞–µ—Ç—Å—è": "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ",
    "—Ö—Ä–∞–Ω–∏—Ç—Å—è": "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ",
    "—Ö—Ä–∞–Ω–µ–Ω–∏–∏": "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ",
}

class ReviewAnalyzer:
    def __init__(self, positive_threshold: int = 4, negative_threshold: int = 2, use_preprocessing: bool = True):
        self.positive_keywords: Set[str] = set()
        self.negative_keywords: Set[str] = set()
        self.positive_threshold = positive_threshold
        self.negative_threshold = negative_threshold
        self.use_preprocessing = use_preprocessing
        # –°–ø–∏—Å–æ–∫ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –∏–ª–∏ –Ω–µ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö —Ñ—Ä–∞–∑
        self.invalid_phrases = {
            "–±–µ–∑–æ–±–∏–¥–Ω—ã–π –≤–∫—É—Å", "–∏–∑—è—â–Ω—ã–π –¥–µ–ª–∞—Ç—å—Å—è", "–≤–∫—É—Å–Ω—ã–π –±–ª–æ–∫", "–¥–æ–ø–æ—Ä—ã–≤–∞—Ç—å",
            "–ø—Ä–∏—Å–ª–∞—Ç—å —Ä–∞–∑–≤–∞–∫—É—É–º", "–≤–æ–∑–≤—Ä–∞—Ç –¥–µ–ª–∞—Ç—å –æ—Ç–∫–∞–∑—ã–≤–∞—Ç—å—Å—è", "–∫–æ—Å–∞—Ä—å –Ω–∞ –≤–µ—Ç–µ—Ä",
            "–ø–∞—Ö–Ω—É—Ç—å –±—ã–ª—å–≥–µ—Ç", "–∑–≤—ã–π —Ä–∞–∑", "–¥–≤–∞ —Ç—ã–∫", "–¥–æ—Ç–æ—à–Ω—ã–π –∞–∫–∫—É–º—É–ª—è—Ç–æ—Ä", "—à–µ–Ω—å 224"
        }
        # –ö—ç—à –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        self.sentiment_cache = {}

    def preprocess_text(self, text: str) -> str:
        """–ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç –æ–ø–µ—á–∞—Ç–∫–∏ –≤ —Ç–µ–∫—Å—Ç–µ, –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Å–ª–æ–≤–∞ –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —ç–º–æ–¥–∑–∏."""
        if not self.use_preprocessing:
            return text
        if speller is None:
            logger.warning("pyaspeller –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç–∫–ª—é—á–µ–Ω–∞.")
            return text
        try:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —ç–º–æ–¥–∑–∏ –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –º–µ—Ç–∫–∏
            emoji_dict = {
                "üòä": "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è",
                "üôÇ": "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è",
                "üòç": "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è",
                "üò¢": "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è",
                "üò°": "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è",
                "üò†": "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è",
            }
            for emoji, label in emoji_dict.items():
                text = text.replace(emoji, f" {label} ")

            # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –æ–ø–µ—á–∞—Ç–∫–∏
            corrected = speller.spelled(text)
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞–º–µ–Ω–∞ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –æ–ø–µ—á–∞—Ç–æ–∫
            corrected = corrected.replace("–ø—Ä–∏ —Ö–∞—Ä–∏", "–ø—Ä–∏ —Ö—Ä–∞–Ω–µ–Ω–∏–∏")
            corrected = corrected.replace("—Ö–∞—Ä–∏", "—Ö—Ä–∞–Ω–µ–Ω–∏–∏")
            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
            doc = nlp(corrected)
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–ª–æ–≤
            normalized_tokens = []
            token_cache = {}  # –õ–æ–∫–∞–ª—å–Ω—ã–π –∫—ç—à –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
            for token in doc:
                if token.is_punct or token.is_stop:
                    normalized_tokens.append(token.text)
                    continue
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–æ–ª—å–∫–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞
                if token.text in token_cache:
                    normalized_tokens.append(token_cache[token.text])
                    continue
                parsed_word = morph.parse(token.text)[0]
                normal_form = parsed_word.normal_form
                if not parsed_word.word:
                    logger.debug(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ —Å–ª–æ–≤–æ: {token.text}")
                    continue
                token_cache[token.text] = normal_form
                normalized_tokens.append(normal_form)
            return " ".join(normalized_tokens)
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –æ–ø–µ—á–∞—Ç–æ–∫: {str(e)}")
            return text

    def split_sentences(self, text: str) -> List[str]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é spaCy, —Å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ—á–∏—Å—Ç–∫–æ–π."""
        text = re.sub(r'[!?.]+\)+|\)+|[:;]-?\)+', '.', text)  # –£—á–∏—Ç—ã–≤–∞–µ–º —Å–º–∞–π–ª–∏–∫–∏ –≤—Ä–æ–¥–µ :) –∏–ª–∏ ;-)
        text = re.sub(r'(\.+|\!+|\?+)', r'. ', text)  # –ó–∞–º–µ–Ω—è–µ–º –º–Ω–æ–≥–æ–∫—Ä–∞—Ç–Ω—ã–µ –∑–Ω–∞–∫–∏ –Ω–∞ –æ–¥–Ω—É —Ç–æ—á–∫—É
        text = re.sub(r'\s+', ' ', text).strip()  # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        doc = nlp(text)
        sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]
        return sentences

    def check_modifiers(self, sentence: str) -> float:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ (–æ—Ç—Ä–∏—Ü–∞–Ω–∏–π, —É—Å–∏–ª–∏—Ç–µ–ª–µ–π, –æ—Å–ª–∞–±–∏—Ç–µ–ª–µ–π) –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–æ—á–Ω—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç."""
        doc = nlp(sentence)
        sentiment_modifier = 1.0
        negation = False
        intensifier = 1.0

        for token in doc:
            if token.lemma_ in ["–Ω–µ", "–Ω–µ—Ç", "–Ω–∏"] and token.dep_ in ["neg"]:
                negation = True
            elif token.lemma_ in ["–æ—á–µ–Ω—å", "–∫—Ä–∞–π–Ω–µ", "—Å–∏–ª—å–Ω–æ"] and token.dep_ in ["advmod"]:
                intensifier = 1.5
            elif token.lemma_ in ["—Å–ª–µ–≥–∫–∞", "–Ω–µ–º–Ω–æ–≥–æ"] and token.dep_ in ["advmod"]:
                intensifier = 0.5

        if negation:
            sentiment_modifier = -1 * intensifier
        else:
            sentiment_modifier = intensifier

        return sentiment_modifier

    def analyze_sentiment_transformers(self, text: str) -> tuple[str, float]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é transformers, —É—á–∏—Ç—ã–≤–∞—è –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã."""
        if not text.strip():
            return '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ', 0.0

        if text in self.sentiment_cache:
            return self.sentiment_cache[text]

        if sentiment_analyzer:
            try:
                result = sentiment_analyzer(text)[0]
                label = result['label'].lower()
                score = result['score']
                doc = nlp(text)
                token_count = len([token for token in doc if not token.is_punct])
                MIN_CONFIDENCE_THRESHOLD = 0.7 if token_count > 5 else 0.5

                if label == "positive" and score > MIN_CONFIDENCE_THRESHOLD:
                    sentiment = "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ"
                    base_score = score
                elif label == "negative" and score > MIN_CONFIDENCE_THRESHOLD:
                    sentiment = "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ"
                    base_score = -score
                else:
                    sentiment = "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ"
                    base_score = 0.0

                modifier = self.check_modifiers(text)
                adjusted_score = base_score * modifier
                if adjusted_score > 0:
                    sentiment = "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ"
                elif adjusted_score < 0:
                    sentiment = "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ"
                else:
                    sentiment = "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ"

                self.sentiment_cache[text] = (sentiment, adjusted_score)
                return sentiment, adjusted_score
            except Exception as e:
                logger.warning(f"Error analyzing sentiment with model: {e}. Falling back to dictionary-based sentiment analysis.")
                return self.fallback_sentiment_analysis(text)
        else:
            return self.fallback_sentiment_analysis(text)

    def fallback_sentiment_analysis(self, text: str) -> tuple[str, float]:
        """–†–µ–∑–µ—Ä–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–ª–æ–≤–∞—Ä—è."""
        doc = nlp(text.lower())
        sentiment_scores = {"–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ": 0, "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ": 0}
        for token in doc:
            sentiment = self.get_sentiment(token.lemma_)
            if sentiment:
                sentiment_scores[sentiment] += 1
        modifier = self.check_modifiers(text)
        if sentiment_scores["–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ"] > sentiment_scores["–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ"]:
            return "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", 0.5 * modifier
        elif sentiment_scores["–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ"] > sentiment_scores["–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ"]:
            return "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.5 * modifier
        else:
            return "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ", 0.0

    def extract_aspects(self, sentence: str, domain_hints: List[str] = None) -> List[tuple]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∞—Å–ø–µ–∫—Ç—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏."""
        doc = nlp(sentence)
        aspects = []
        invalid_words = {"–æ—á–µ–Ω–∫", "–ø–æ—Ä–æ–±–æ–≤–∞—Ç—å", "—Å–ø–∞—Å–Ω–æ–≥–æ", "–∑–∞—Å–∫—É—á–∞—Ç—Å—è", "–¥–æ–±–æ–≤—Å—Ç–≤–æ", "—Ö–∞—Ä–∏"}
        invalid_phrases = ["–ª—É—á—à–µ –≤—Ä–µ–º–µ–Ω–∏", "—Ä–∞–∑–æ–º —Å–µ–±—è", "–ø—Ä–∏ —Ö–∞—Ä–∏"]

        domain = "—Ç–µ—Ö–Ω–∏–∫–∞" if domain_hints and any(hint in ["—ç–∫—Ä–∞–Ω", "–∫–∞–º–µ—Ä–∞", "–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä"] for hint in domain_hints) else "–µ–¥–∞" if domain_hints and any(hint in ["—Ä—ã–±–∞", "–º—è—Å–æ", "–µ–¥–∞", "–∫—Ä–µ–≤–µ—Ç–∫–∞", "–º–∏–Ω—Ç–∞–π"] for hint in domain_hints) else "–æ–±—â–∏–π"

        for token in doc:
            if token.pos_ in ["NOUN", "VERB", "ADJ"] and token.lemma_ not in invalid_words:
                aspect = token.lemma_
                modifiers = []
                negation = False
                for child in token.children:
                    if child.dep_ in ["amod", "compound", "advmod"] and child.lemma_ not in invalid_words:
                        modifiers.append(child.text)
                    if child.lemma_ in ["–Ω–µ", "–Ω–µ—Ç", "–Ω–∏"] and child.dep_ in ["neg"]:
                        negation = True
                aspect_phrase = " ".join(modifiers + [aspect])
                if any(phrase in aspect_phrase.lower() for phrase in invalid_phrases) or aspect_phrase.lower() in self.invalid_phrases:
                    continue

                sentiment, score = self.analyze_sentiment_transformers(aspect_phrase)
                if negation:
                    if sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ" and any(mod in ["–≤–∫—É—Å–Ω—ã–π", "—Ö–æ—Ä–æ—à–∏–π", "–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω—ã–π"] for mod in modifiers):
                        sentiment = "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ"
                        score = -score
                    elif sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ":
                        sentiment = "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ"
                        score = -score
                if domain == "–µ–¥–∞" and aspect in ["—Ö—Ä–∞–Ω–∏—Ç—Å—è"] and "–Ω–µ" in sentence.lower():
                    sentiment = "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ"
                    score = -abs(score)
                elif any(word in aspect_phrase.lower() for word in ["–≤–∫—É—Å–Ω—ã–π", "—Ö–æ—Ä–æ—à–∏–π", "—Å–≤–µ–∂–∏–π", "–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω—ã–π"]) and sentiment != "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ":
                    sentiment = "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ"
                    score = abs(score)
                elif any(word in aspect_phrase.lower() for word in ["–±–µ–∑–≤–∫—É—Å–Ω—ã–π", "–ø—Ä–æ–≥–æ—Ä–∫–ª—ã–π", "–ø—Ä–æ—Ç—É—Ö—à–∏–π", "—Ö—É–¥—à–∏–π", "—Ç—Ä—ã–Ω–¥–µ—Ü"]):
                    sentiment = "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ"
                    score = -abs(score)

                aspects.append((aspect_phrase, sentiment, score))

        return aspects

    def analyze_review_sentences(self, review_text: str, domain_hints: List[str] = None) -> List[tuple]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ –æ—Ç–∑—ã–≤–µ –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∞—Å–ø–µ–∫—Ç—ã."""
        review_text = self.preprocess_text(review_text)
        sentences = self.split_sentences(review_text)
        result = []

        for sentence in sentences:
            doc = nlp(sentence)
            clauses = []
            current_clause = []
            for token in doc:
                current_clause.append(token.text)
                if token.dep_ in ["cc", "punct"] and token.lemma_ in ["–∏", "–Ω–æ", "–∞", "–∏–ª–∏", ","]:
                    if current_clause:
                        clauses.append(" ".join(current_clause).strip())
                        current_clause = []
            if current_clause:
                clauses.append(" ".join(current_clause).strip())

            clause_sentiments = self.analyze_batch_sentiments(clauses)
            for clause, (sentiment, score) in zip(clauses, clause_sentiments):
                aspects = self.extract_aspects(clause, domain_hints)
                result.append((clause, sentiment, score, aspects))

        return result

    def analyze_batch_sentiments(self, texts: List[str]) -> List[tuple]:
        """–ü–∞–∫–µ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–æ–≤."""
        if not texts:
            return [('–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ', 0.0) for _ in texts]

        results = []
        uncached_texts = []
        uncached_indices = []
        for i, text in enumerate(texts):
            if text in self.sentiment_cache:
                results.append(self.sentiment_cache[text])
            else:
                uncached_texts.append(text)
                uncached_indices.append(i)
                results.append(None)

        if uncached_texts:
            if sentiment_analyzer:
                try:
                    batch_results = sentiment_analyzer(uncached_texts)
                    for idx, res in zip(uncached_indices, batch_results):
                        label = res['label'].lower()
                        score = res['score']
                        doc = nlp(uncached_texts[idx - uncached_indices[0]])
                        token_count = len([token for token in doc if not token.is_punct])
                        MIN_CONFIDENCE_THRESHOLD = 0.7 if token_count > 5 else 0.5

                        if label == "positive" and score > MIN_CONFIDENCE_THRESHOLD:
                            sentiment = "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ"
                            base_score = score
                        elif label == "negative" and score > MIN_CONFIDENCE_THRESHOLD:
                            sentiment = "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ"
                            base_score = -score
                        else:
                            sentiment = "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ"
                            base_score = 0.0
                        modifier = self.check_modifiers(uncached_texts[idx - uncached_indices[0]])
                        adjusted_score = base_score * modifier
                        if adjusted_score > 0:
                            sentiment = "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ"
                        elif adjusted_score < 0:
                            sentiment = "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ"
                        else:
                            sentiment = "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ"
                        results[idx] = (sentiment, adjusted_score)
                        self.sentiment_cache[uncached_texts[idx - uncached_indices[0]]] = (sentiment, adjusted_score)
                except Exception as e:
                    logger.warning(f"Error in batch sentiment analysis: {e}. Falling back to dictionary-based sentiment analysis.")
                    for idx, text in zip(uncached_indices, uncached_texts):
                        sentiment, score = self.fallback_sentiment_analysis(text)
                        results[idx] = (sentiment, score)
                        self.sentiment_cache[text] = (sentiment, score)
            else:
                for idx, text in zip(uncached_indices, uncached_texts):
                    sentiment, score = self.fallback_sentiment_analysis(text)
                    results[idx] = (sentiment, score)
                    self.sentiment_cache[text] = (sentiment, score)

        return results

    def analyze_reviews(self, csv_path: str) -> Dict[str, str]:
        try:
            df = pd.read_csv(csv_path, encoding='utf-8')
            site_keywords: List[str] = []
            positive_count = 0
            negative_count = 0
            positive_aspects = Counter()
            negative_aspects = Counter()
            self.positive_keywords.clear()
            self.negative_keywords.clear()
            processed_texts_set = set()

            domain_hints = []
            if all(col in df.columns for col in ['–î–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤–∞', '–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏', '–û—Ü–µ–Ω–∫–∞']):
                texts = [(str(row['–î–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤–∞']) if pd.notna(row['–î–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤–∞']) else "",
                          str(row['–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏']) if pd.notna(row['–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏']) else "",
                          int(row['–û—Ü–µ–Ω–∫–∞']) if pd.notna(row['–û—Ü–µ–Ω–∫–∞']) else 3)
                         for _, row in df.iterrows()]
                for pros_text, cons_text, _ in texts:
                    if pros_text.strip() and pros_text.strip().lower() != "–Ω–µ—Ç":
                        doc = nlp(pros_text.lower())
                        for token in doc:
                            if token.lemma_ in ["—Ä—ã–±–∞", "–º—è—Å–æ", "–µ–¥–∞", "—ç–∫—Ä–∞–Ω", "–∫–∞–º–µ—Ä–∞", "–∫—Ä–µ–≤–µ—Ç–∫–∞", "–º–∏–Ω—Ç–∞–π"]:
                                domain_hints.append(token.lemma_)
                    if cons_text.strip() and cons_text.strip().lower() != "–Ω–µ—Ç":
                        doc = nlp(cons_text.lower())
                        for token in doc:
                            if token.lemma_ in ["—Ä—ã–±–∞", "–º—è—Å–æ", "–µ–¥–∞", "—ç–∫—Ä–∞–Ω", "–∫–∞–º–µ—Ä–∞", "–∫—Ä–µ–≤–µ—Ç–∫–∞", "–º–∏–Ω—Ç–∞–π"]:
                                domain_hints.append(token.lemma_)
            elif all(col in df.columns for col in ['–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞', '–û—Ü–µ–Ω–∫–∞']):
                texts = [str(row['–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞']) for _, row in df.iterrows()]
                for text in texts:
                    doc = nlp(text.lower())
                    for token in doc:
                        if token.lemma_ in ["—Ä—ã–±–∞", "–º—è—Å–æ", "–µ–¥–∞", "—ç–∫—Ä–∞–Ω", "–∫–∞–º–µ—Ä–∞", "–∫—Ä–µ–≤–µ—Ç–∫–∞", "–º–∏–Ω—Ç–∞–π"]:
                            domain_hints.append(token.lemma_)

            domain_hints = list(set(domain_hints))

            if all(col in df.columns for col in ['–î–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤–∞', '–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏', '–û—Ü–µ–Ω–∫–∞']):
                texts = [(str(row['–î–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤–∞']) if pd.notna(row['–î–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤–∞']) else "",
                          str(row['–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏']) if pd.notna(row['–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏']) else "",
                          int(row['–û—Ü–µ–Ω–∫–∞']) if pd.notna(row['–û—Ü–µ–Ω–∫–∞']) else 3)
                         for _, row in df.iterrows()]
                pros_texts, cons_texts, ratings = zip(*texts)
                pros_processed = self.process_texts(pros_texts, domain_hints)
                cons_processed = self.process_texts(cons_texts, domain_hints)

                for (pros_text, cons_text, rating), pros_keywords, cons_keywords in zip(texts, pros_processed, cons_processed):
                    site_keywords.extend(pros_keywords + cons_keywords)
                    if pros_text.strip() and pros_text.strip().lower() != "–Ω–µ—Ç" and pros_text not in processed_texts_set:
                        processed_texts_set.add(pros_text)
                        pros_sentences = self.analyze_review_sentences(pros_text, domain_hints)
                        pros_overall_sentiment, _ = self.analyze_sentiment_transformers(pros_text)
                        for sentence, sentiment, _, aspects in pros_sentences:
                            if sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ":
                                if aspects:
                                    for aspect_phrase, aspect_sentiment, _ in aspects:
                                        if aspect_sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ":
                                            positive_aspects[aspect_phrase] += 1
                                            self.positive_keywords.add(aspect_phrase)
                                        elif aspect_sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ":
                                            negative_aspects[aspect_phrase] += 1
                                            self.negative_keywords.add(aspect_phrase)
                                else:
                                    positive_aspects[sentence] += 1
                                    self.positive_keywords.add(sentence)
                            elif sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ":
                                if aspects:
                                    for aspect_phrase, aspect_sentiment, _ in aspects:
                                        if aspect_sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ":
                                            positive_aspects[aspect_phrase] += 1
                                            self.positive_keywords.add(aspect_phrase)
                                        elif aspect_sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ":
                                            negative_aspects[aspect_phrase] += 1
                                            self.negative_keywords.add(aspect_phrase)
                                else:
                                    negative_aspects[sentence] += 1
                                    self.negative_keywords.add(sentence)
                        if pros_overall_sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ" and not positive_aspects:
                            positive_aspects[pros_text] += 1
                            self.positive_keywords.add(pros_text)
                        elif pros_overall_sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ" and not negative_aspects:
                            negative_aspects[pros_text] += 1
                            self.negative_keywords.add(pros_text)

                    if cons_text.strip() and cons_text.strip().lower() != "–Ω–µ—Ç" and cons_text not in processed_texts_set:
                        processed_texts_set.add(cons_text)
                        cons_sentences = self.analyze_review_sentences(cons_text, domain_hints)
                        cons_overall_sentiment, _ = self.analyze_sentiment_transformers(cons_text)
                        for sentence, sentiment, _, aspects in cons_sentences:
                            if sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ":
                                if aspects:
                                    for aspect_phrase, aspect_sentiment, _ in aspects:
                                        if aspect_sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ":
                                            positive_aspects[aspect_phrase] += 1
                                            self.positive_keywords.add(aspect_phrase)
                                        elif aspect_sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ":
                                            negative_aspects[aspect_phrase] += 1
                                            self.negative_keywords.add(aspect_phrase)
                                else:
                                    positive_aspects[sentence] += 1
                                    self.positive_keywords.add(sentence)
                            elif sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ":
                                if aspects:
                                    for aspect_phrase, aspect_sentiment, _ in aspects:
                                        if aspect_sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ":
                                            positive_aspects[aspect_phrase] += 1
                                            self.positive_keywords.add(aspect_phrase)
                                        elif aspect_sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ":
                                            negative_aspects[aspect_phrase] += 1
                                            self.negative_keywords.add(aspect_phrase)
                                else:
                                    negative_aspects[sentence] += 1
                                    self.negative_keywords.add(sentence)
                        if cons_overall_sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ" and not positive_aspects:
                            positive_aspects[cons_text] += 1
                            self.positive_keywords.add(cons_text)
                        elif cons_overall_sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ" and not negative_aspects:
                            negative_aspects[cons_text] += 1
                            self.negative_keywords.add(cons_text)

                    if rating >= self.positive_threshold:
                        positive_count += 1
                    elif rating <= self.negative_threshold:
                        negative_count += 1

            elif all(col in df.columns for col in ['–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞', '–û—Ü–µ–Ω–∫–∞']):
                texts = [str(row['–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞']) for _, row in df.iterrows()]
                ratings = [int(row['–û—Ü–µ–Ω–∫–∞']) if pd.notna(row['–û—Ü–µ–Ω–∫–∞']) else 3 for _, row in df.iterrows()]
                processed_texts = self.process_texts(texts, domain_hints)

                for text, rating, keywords in zip(texts, ratings, processed_texts):
                    site_keywords.extend(keywords)
                    if text not in processed_texts_set:
                        processed_texts_set.add(text)
                        sentences = self.analyze_review_sentences(text, domain_hints)
                        text_overall_sentiment, _ = self.analyze_sentiment_transformers(text)
                        for sentence, sentiment, _, aspects in sentences:
                            if sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ":
                                if aspects:
                                    for aspect_phrase, aspect_sentiment, _ in aspects:
                                        if aspect_sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ":
                                            positive_aspects[aspect_phrase] += 1
                                            self.positive_keywords.add(aspect_phrase)
                                        elif aspect_sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ":
                                            negative_aspects[aspect_phrase] += 1
                                            self.negative_keywords.add(aspect_phrase)
                                else:
                                    positive_aspects[sentence] += 1
                                    self.positive_keywords.add(sentence)
                            elif sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ":
                                if aspects:
                                    for aspect_phrase, aspect_sentiment, _ in aspects:
                                        if aspect_sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ":
                                            positive_aspects[aspect_phrase] += 1
                                            self.positive_keywords.add(aspect_phrase)
                                        elif aspect_sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ":
                                            negative_aspects[aspect_phrase] += 1
                                            self.negative_keywords.add(aspect_phrase)
                                else:
                                    negative_aspects[sentence] += 1
                                    self.negative_keywords.add(sentence)
                        if text_overall_sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ" and not positive_aspects:
                            positive_aspects[text] += 1
                            self.positive_keywords.add(text)
                        elif text_overall_sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ" and not negative_aspects:
                            negative_aspects[text] += 1
                            self.negative_keywords.add(text)
                    if rating >= self.positive_threshold:
                        positive_count += 1
                    elif rating <= self.negative_threshold:
                        negative_count += 1

            else:
                raise ValueError("CSV-—Ñ–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –ª–∏–±–æ —Å—Ç–æ–ª–±—Ü—ã '–î–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤–∞', '–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏' –∏ '–û—Ü–µ–Ω–∫–∞', –ª–∏–±–æ '–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞' –∏ '–û—Ü–µ–Ω–∫–∞'")

            # –£–¥–∞–ª—è–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è –º–µ–∂–¥—É –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º–∏ –∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º–∏ –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
            common_keywords = self.positive_keywords.intersection(self.negative_keywords)
            for keyword in common_keywords:
                sentiment = self.get_sentiment(keyword)
                if sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ":
                    self.negative_keywords.discard(keyword)
                elif sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ":
                    self.positive_keywords.discard(keyword)
                else:
                    pos_count = sum(1 for aspect, count in positive_aspects.items() if keyword in aspect for _ in range(count))
                    neg_count = sum(1 for aspect, count in negative_aspects.items() if keyword in aspect for _ in range(count))
                    if pos_count > neg_count:
                        self.negative_keywords.discard(keyword)
                    else:
                        self.positive_keywords.discard(keyword)

            main_positives = "\n".join([f"{aspect} (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ) ({count})" for aspect, count in positive_aspects.most_common(5)]) if positive_aspects else "–ù–µ—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤"
            main_negatives = "\n".join([f"{aspect} (–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ) ({count})" for aspect, count in negative_aspects.most_common(5)]) if negative_aspects else "–ù–µ—Ç –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤"
            top_keywords = self.get_top_keywords(site_keywords, 15)

            return {
                "–ü–ª—é—Å—ã": main_positives,
                "–ú–∏–Ω—É—Å—ã": main_negatives,
                "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ)": ", ".join(sorted(self.positive_keywords)[:10]),
                "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ)": ", ".join(sorted(self.negative_keywords)[:10]),
                "–û–±—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞": ", ".join(top_keywords.split(", ")),
                "–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": str(positive_count),
                "–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": str(negative_count)
            }
        except UnicodeDecodeError as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ {csv_path}: {str(e)}")
            return {
                "–ü–ª—é—Å—ã": "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏",
                "–ú–∏–Ω—É—Å—ã": "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏",
                "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ)": "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏",
                "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ)": "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏",
                "–û–±—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞": "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏",
                "–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏",
                "–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏"
            }
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Ñ–∞–π–ª–∞ {csv_path}: {str(e)}")
            return {
                "–ü–ª—é—Å—ã": "–û—à–∏–±–∫–∞",
                "–ú–∏–Ω—É—Å—ã": "–û—à–∏–±–∫–∞",
                "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ)": "–û—à–∏–±–∫–∞",
                "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ)": "–û—à–∏–±–∫–∞",
                "–û–±—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞": "–û—à–∏–±–∫–∞",
                "–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": "–û—à–∏–±–∫–∞",
                "–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": "–û—à–∏–±–∫–∞"
            }

    def aggregate_reviews(self, csv_paths: List[str]) -> Dict[str, str]:
        """–ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ—Ç–∑—ã–≤–æ–≤ –∏–∑ –≤—Å–µ—Ö CSV-—Ñ–∞–π–ª–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç–∏."""
        all_keywords: List[str] = []
        total_positive_count = 0
        total_negative_count = 0
        all_positive_aspects = Counter()
        all_negative_aspects = Counter()

        def process_single_file(csv_path):
            result = self.analyze_reviews(csv_path)
            keywords = result["–û–±—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞"].split(", ")
            positive_count = int(result["–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã"]) if result["–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã"].isdigit() else 0
            negative_count = int(result["–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã"]) if result["–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã"].isdigit() else 0
            return keywords, positive_count, negative_count

        with ThreadPoolExecutor(max_workers=4) as executor:
            results = list(executor.map(process_single_file, csv_paths))

        for keywords, positive_count, negative_count in results:
            all_keywords.extend(keywords)
            total_positive_count += positive_count
            total_negative_count += negative_count

        for csv_path in csv_paths:
            df = pd.read_csv(csv_path, encoding='utf-8')
            if all(col in df.columns for col in ['–î–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤–∞', '–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏', '–û—Ü–µ–Ω–∫–∞']):
                texts = [(str(row['–î–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤–∞']) if pd.notna(row['–î–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤–∞']) else "",
                          str(row['–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏']) if pd.notna(row['–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏']) else "",
                          int(row['–û—Ü–µ–Ω–∫–∞']) if pd.notna(row['–û—Ü–µ–Ω–∫–∞']) else 3)
                         for _, row in df.iterrows()]
                for pros_text, cons_text, _ in texts:
                    if pros_text.strip() and pros_text.strip().lower() != "–Ω–µ—Ç":
                        pros_sentences = self.analyze_review_sentences(pros_text)
                        for sentence, sentiment, _, aspects in pros_sentences:
                            if sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ":
                                for aspect_phrase, aspect_sentiment, _ in aspects:
                                    if aspect_sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ":
                                        all_positive_aspects[aspect_phrase] += 1
                                        self.positive_keywords.add(aspect_phrase)
                            elif sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ":
                                for aspect_phrase, aspect_sentiment, _ in aspects:
                                    if aspect_sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ":
                                        all_negative_aspects[aspect_phrase] += 1
                                        self.negative_keywords.add(aspect_phrase)
                    if cons_text.strip() and cons_text.strip().lower() != "–Ω–µ—Ç":
                        cons_sentences = self.analyze_review_sentences(cons_text)
                        for sentence, sentiment, _, aspects in cons_sentences:
                            if sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ":
                                for aspect_phrase, aspect_sentiment, _ in aspects:
                                    if aspect_sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ":
                                        all_positive_aspects[aspect_phrase] += 1
                                        self.positive_keywords.add(aspect_phrase)
                            elif sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ":
                                for aspect_phrase, aspect_sentiment, _ in aspects:
                                    if aspect_sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ":
                                        all_negative_aspects[aspect_phrase] += 1
                                        self.negative_keywords.add(aspect_phrase)

            elif all(col in df.columns for col in ['–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞', '–û—Ü–µ–Ω–∫–∞']):
                texts = [str(row['–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞']) for _, row in df.iterrows()]
                for text in texts:
                    sentences = self.analyze_review_sentences(text)
                    for sentence, sentiment, _, aspects in sentences:
                        if sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ":
                            for aspect_phrase, aspect_sentiment, _ in aspects:
                                if aspect_sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ":
                                    all_positive_aspects[aspect_phrase] += 1
                                    self.positive_keywords.add(aspect_phrase)
                        elif sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ":
                            for aspect_phrase, aspect_sentiment, _ in aspects:
                                if aspect_sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ":
                                    all_negative_aspects[aspect_phrase] += 1
                                    self.negative_keywords.add(aspect_phrase)

        main_positives = "\n".join([f"{aspect} (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ) ({count})" for aspect, count in all_positive_aspects.most_common(5)]) if all_positive_aspects else "–ù–µ—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤"
        main_negatives = "\n".join([f"{aspect} (–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ) ({count})" for aspect, count in all_negative_aspects.most_common(5)]) if all_negative_aspects else "–ù–µ—Ç –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤"
        top_keywords = self.get_top_keywords(all_keywords, 15)

        return {
            "–ü–ª—é—Å—ã (–≤—Å–µ —Å–∞–π—Ç—ã)": main_positives,
            "–ú–∏–Ω—É—Å—ã (–≤—Å–µ —Å–∞–π—Ç—ã)": main_negatives,
            "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ, –≤—Å–µ —Å–∞–π—Ç—ã)": ", ".join(sorted(self.positive_keywords)[:10]),
            "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ, –≤—Å–µ —Å–∞–π—Ç—ã)": ", ".join(sorted(self.negative_keywords)[:10]),
            "–û–±—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–≤—Å–µ —Å–∞–π—Ç—ã)": ", ".join(top_keywords.split(", ")),
            "–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã (–≤—Å–µ —Å–∞–π—Ç—ã)": str(total_positive_count),
            "–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã (–≤—Å–µ —Å–∞–π—Ç—ã)": str(total_negative_count)
        }

    def process_texts(self, texts: List[str], domain_hints: List[str] = None) -> List[List[str]]:
        """–ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é spaCy —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π —à—É–º–Ω—ã—Ö —Å–ª–æ–≤ –∏ —É—á—ë—Ç–æ–º –¥–æ–º–µ–Ω–∞."""
        try:
            keywords_list = []
            cyrillic_pattern = re.compile(r'^[–∞-—è–ê-–Ø—ë–Å]+$')
            texts = [self.preprocess_text(text.lower()) for text in texts]

            domain = "—Ç–µ—Ö–Ω–∏–∫–∞" if domain_hints and any(hint in ["—ç–∫—Ä–∞–Ω", "–∫–∞–º–µ—Ä–∞", "–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä"] for hint in domain_hints) else "–µ–¥–∞" if domain_hints and any(hint in ["—Ä—ã–±–∞", "–º—è—Å–æ", "–µ–¥–∞"] for hint in domain_hints) else "–æ–±—â–∏–π"
            
            tech_words = {"—ç–∫—Ä–∞–Ω", "–∫–∞–º–µ—Ä–∞", "–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä", "—Ç–æ—Ä–º–æ–∑–∏—Ç", "–≥–ª—é—á–Ω—ã–π", "—è—Ä–∫–∏–π"}
            food_words = {"–≤–∫—É—Å–Ω—ã–π", "—Å–≤–µ–∂–∏–π", "–ø—Ä–æ–≥–æ—Ä–∫–ª—ã–π", "—Ö—Ä–∞–Ω–∏—Ç—Å—è", "–ø–∞—Ö–Ω–µ—Ç", "–º–∏–Ω—Ç–∞–π", "–∫—Ä–µ–≤–µ—Ç–∫–∞"}

            action_verbs = {"–ø–æ—Ö–≤–∞–ª–∏—Ç—å", "–∫—É–ø–∏—Ç—å", "–ø—Ä–∏–≥–æ—Ç–æ–≤–∏—Ç—å", "—Å—ä–µ—Å—Ç—å"}

            for doc in nlp.pipe(texts, disable=["ner"]):
                keywords = []
                for token in doc:
                    lemma = token.lemma_
                    if (not token.is_stop and not token.is_punct and
                        token.pos_ in ["NOUN", "ADJ", "ADV"] and
                        len(lemma) >= 3 and
                        cyrillic_pattern.match(lemma)):
                        if lemma in action_verbs:
                            continue
                        if domain == "–µ–¥–∞" and lemma in tech_words:
                            continue
                        if domain == "—Ç–µ—Ö–Ω–∏–∫–∞" and lemma in food_words:
                            continue
                        keywords.append(lemma)
                    elif token.pos_ == "VERB" and lemma in food_words and domain == "–µ–¥–∞":
                        keywords.append(lemma)
                keywords_list.append(keywords)
            return keywords_list
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–æ–≤: {str(e)}")
            return [[] for _ in texts]

    def get_sentiment(self, word: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Å–ª–æ–≤–∞ —Å –ø–æ–º–æ—â—å—é —Å–ª–æ–≤–∞—Ä—è SENTIMENT_DICT."""
        return SENTIMENT_DICT.get(word, "")

    def summarize_reviews(self, reviews: List[Dict[str, Union[str, List[str]]]]) -> str:
        """–°—É–º–º–∏—Ä—É–µ—Ç –æ—Ç–∑—ã–≤—ã —Å —É—á—ë—Ç–æ–º —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∏ –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏."""
        try:
            if not reviews:
                return "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏—è"
            all_keywords = [keyword for review in reviews for keyword in review["keywords"]]
            keyword_counts = Counter(all_keywords)
            top_keywords = [(word, count) for word, count in keyword_counts.most_common(10) if count >= 1]

            summary = []
            for word, count in top_keywords:
                sentiment = self.get_sentiment(word)
                sentiment_label = f" ({sentiment})" if sentiment else ""
                summary.append(f"{word}{sentiment_label} ({count})")
            return "\n".join(summary) if summary else "–ù–µ—Ç –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏—è"
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏ –æ—Ç–∑—ã–≤–æ–≤: {str(e)}")
            return "–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏"

    def get_top_keywords(self, keywords: List[str], n: int = 15) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ø-n –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –ø–æ —á–∞—Å—Ç–æ—Ç–µ."""
        try:
            if not keywords:
                return "–ù–µ—Ç –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"
            keyword_counts = Counter(keywords)
            top_keywords = [word for word, count in keyword_counts.most_common(n) if count >= 1]
            return ", ".join(top_keywords) if top_keywords else "–ù–µ—Ç –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Ç–æ–ø-–∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {str(e)}")
            return "–û—à–∏–±–∫–∞"
