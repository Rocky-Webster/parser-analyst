import pandas as pd
import spacy
from collections import Counter
import logging
import sys
from typing import List, Dict, Set, Union, Tuple
import re
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
import pymorphy3
from concurrent.futures import ThreadPoolExecutor
import numpy as np
from datetime import datetime
import json

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ YandexSpeller
try:
    from pyaspeller import YandexSpeller
    speller = YandexSpeller()
except ImportError:
    logging.warning("–ú–æ–¥—É–ª—å pyaspeller –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –±—É–¥–µ—Ç –æ—Ç–∫–ª—é—á–µ–Ω–∞.")
    speller = None

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('review_analyzer.log', encoding='utf-8')
    ],
    encoding='utf-8-sig'
)
logger = logging.getLogger(__name__)

# –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –∑–∞–º–µ–Ω–∞ —ç–º–æ–¥–∑–∏
def replace_emoji_for_logging(text: str) -> str:
    emoji_dict = {
        "üëç": "[–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π_—ç–º–æ–¥–∑–∏]", "üëå": "[–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π_—ç–º–æ–¥–∑–∏]", "üòä": "[–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è]",
        "üôÇ": "[–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è]", "üòç": "[–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è]", "üò¢": "[–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è]",
        "üò°": "[–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è]", "üò†": "[–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è]", "üòã": "[–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è]",
        "ÔºÅ": "–≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏–µ", "‚Äº": "–¥–≤–æ–π–Ω–æ–µ_–≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏–µ", "Ôºü": "–≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π_–∑–Ω–∞–∫",
        "üåª": "—Ü–≤–µ—Ç–æ–∫_—ç–º–æ–¥–∑–∏", "üöò": "–º–∞—à–∏–Ω–∞_—ç–º–æ–¥–∑–∏", "üëé": "[–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π_—ç–º–æ–¥–∑–∏]",
        "üòû": "[–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è]", "ü§î": "[–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è]", "üí°": "[–∏–¥–µ—è_—ç–º–æ–¥–∑–∏]"
    }
    for emoji, label in emoji_dict.items():
        text = text.replace(emoji, f" {label} ")
    return text.strip()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è spaCy –∏ pymorphy3
nlp = spacy.load("ru_core_news_sm", disable=["ner"])
morph = pymorphy3.MorphAnalyzer()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
model_name = "seara/rubert-tiny2-russian-sentiment"
sentiment_analyzer = None
try:
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name)
    sentiment_analyzer = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
    logger.info(f"–ú–æ–¥–µ–ª—å {model_name} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")
except Exception as e:
    logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}: {e}. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å–ª–æ–≤–∞—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑.")
    sentiment_analyzer = None

# –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
SENTIMENT_DICT = {
    "—Ö–æ—Ä–æ—à–∏–π": ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", 0.7), "–æ—Ç–ª–∏—á–Ω—ã–π": ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", 0.9), "–ø—Ä–µ–∫—Ä–∞—Å–Ω—ã–π": ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", 0.8),
    "—É–¥–æ–±–Ω—ã–π": ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", 0.6), "–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π": ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", 0.7), "–±—ã—Å—Ç—Ä—ã–π": ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", 0.6),
    "–Ω–∞–¥–µ–∂–Ω—ã–π": ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", 0.7), "–≤–∫—É—Å–Ω—ã–π": ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", 0.7), "–∫—Ä–∞—Å–∏–≤—ã–π": ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", 0.6),
    "–ø—Ä–∏—è—Ç–Ω—ã–π": ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", 0.6), "–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω—ã–π": ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", 0.8), "—Å–≤–µ–∂–∏–π": ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", 0.6),
    "–≤–µ–∂–ª–∏–≤—ã–π": ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", 0.7), "–ø–ª–æ—Ö–æ–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.7), "—É–∂–∞—Å–Ω—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.9),
    "–¥–µ—Ñ–µ–∫—Ç–Ω—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.8), "–Ω–µ—É–¥–æ–±–Ω—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.6), "–º–µ–¥–ª–µ–Ω–Ω—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.6),
    "–Ω–µ–Ω–∞–¥–µ–∂–Ω—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.7), "–Ω–µ–≤–∫—É—Å–Ω—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.7), "—Å–ª–æ–º–∞–Ω–Ω—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.8),
    "–≥—Ä—è–∑–Ω—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.7), "–¥–æ—Ä–æ–≥–æ–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.7), "–±–µ–∑–≤–∫—É—Å–Ω—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.7),
    "–ø—Ä–æ–≥–æ—Ä–∫–ª—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.8), "–ø—Ä–æ—Ç—É—Ö—à–∏–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.9), "—Ç—Ä—ã–Ω–¥–µ—Ü": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.9),
    "—Ö—É–¥—à–∏–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.8), "—Ö–∞–º–æ–≤–∞—Ç—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.7), "–Ω–µ–∫–≤–∞–ª–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.7),
    "–≤—ã—Å–æ–∫–∏–π": ("–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ", 0.0), "–ø—Ä–æ—Å—Ç–æ": ("–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ", 0.0), "—Ä–∞–∑–≤–∏–≤–∞–µ—Ç—Å—è": ("–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ", 0.0),
    "—Ö—Ä–∞–Ω–∏—Ç—Å—è": ("–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ", 0.0), "—Ö—Ä–∞–Ω–µ–Ω–∏–∏": ("–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ", 0.0), "–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ": ("–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ", 0.0),
    "–ø–µ—Ä—Å–æ–Ω–∞–ª": ("–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ", 0.0), "—Ç–æ–ø–ª–∏–≤–æ": ("–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ", 0.0), "–Ω–µ—Ç": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.5),
    "–Ω–µ—Ç—É": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.5), "–æ—á–µ–Ω—å": ("—É—Å–∏–ª–∏—Ç–µ–ª—å", 1.5), "—Å–ª–µ–≥–∫–∞": ("–æ—Å–ª–∞–±–∏—Ç–µ–ª—å", 0.5),
    "—Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ": ("—É—Å–∏–ª–∏—Ç–µ–ª—å", 1.3), "–∞–±—Å–æ–ª—é—Ç–Ω–æ": ("—É—Å–∏–ª–∏—Ç–µ–ª—å", 1.4), "–Ω–µ–º–Ω–æ–≥–æ": ("–æ—Å–ª–∞–±–∏—Ç–µ–ª—å", 0.7),
    "–¥—Ä—è–Ω—å": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.8), "–æ–±–º–∞–Ω": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.7), "–≤–æ—Ä—É—é—Ç": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.9),
    "—Ö–º—É—Ä—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.6), "–Ω–µ–¥–æ—Å–º–æ—Ç—Ä": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.6), "–æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω–æ": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.9),
    "–≤–æ—Ä–æ–≤—Å—Ç–≤–æ": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.9), "–æ–±–º–∞–Ω—ã–≤–∞—é—Ç": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.8), "–Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.7),
    "–∂–µ—Å—Ç—å": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.7), "—Å—Ç—ã–¥–æ–±–µ–Ω—å": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.7), "–Ω–µ–¥–æ–ª–∏–≤": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.8),
    "—Ö–∞–º—Å—Ç–≤–æ": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.7), "–¥–æ—Ä–æ–≥–æ–≤–∞—Ç–æ": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.8), "–º–∞–ª–æ": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.6),
    "–º–∞–ª–µ–Ω—å–∫–∏–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.5), "–º–∞–ª–æ–≤–∞—Ç–æ": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.6), "—Å—Ç—Ä–∞–Ω–Ω—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.6),
    "–≤—ã—Å–æ–∫–∞—è": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.6), "–Ω–µ–ø–æ–Ω—è—Ç–Ω—ã–π": ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", -0.6)
}

class ReviewAnalyzer:
    def __init__(self, use_preprocessing: bool = True, positive_threshold: int = 4, negative_threshold: int = 2):
        self.positive_keywords: Set[str] = set()
        self.negative_keywords: Set[str] = set()
        self.positive_threshold = positive_threshold
        self.negative_threshold = negative_threshold
        self.use_preprocessing = use_preprocessing
        self.invalid_phrases = {
            "–±–µ–∑–æ–±–∏–¥–Ω—ã–π –≤–∫—É—Å", "–∏–∑—è—â–Ω—ã–π –¥–µ–ª–∞—Ç—å—Å—è", "–≤–∫—É—Å–Ω—ã–π –±–ª–æ–∫", "–¥–æ–ø–æ—Ä—ã–≤–∞—Ç—å",
            "–ø—Ä–∏—Å–ª–∞—Ç—å —Ä–∞–∑–≤–∞–∫—É—É–º", "–≤–æ–∑–≤—Ä–∞—Ç –¥–µ–ª–∞—Ç—å –æ—Ç–∫–∞–∑—ã–≤–∞—Ç—å—Å—è", "–∫–æ—Å–∞—Ä—å –Ω–∞ –≤–µ—Ç–µ—Ä",
            "–ø–∞—Ö–Ω—É—Ç—å –±—ã–ª—å–≥–µ—Ç", "–∑–≤—ã–π —Ä–∞–∑", "–¥–≤–∞ —Ç—ã–∫", "–¥–æ—Ç–æ—à–Ω—ã–π –∞–∫–∫—É–º—É–ª—è—Ç–æ—Ä", "—à–µ–Ω—å 224"
        }
        self.sentiment_cache = {}
        self.domain_hints = [
            "—Ä—ã–±–∞", "–º—è—Å–æ", "–µ–¥–∞", "—ç–∫—Ä–∞–Ω", "–∫–∞–º–µ—Ä–∞", "–∫—Ä–µ–≤–µ—Ç–∫–∞", "–º–∏–Ω—Ç–∞–π", "–≥—Ä–µ–±–µ—à–æ–∫", "–≤–∫—É—Å", "–∫–∞—á–µ—Å—Ç–≤–æ", "—Ä–∞–∑–º–µ—Ä",
            "—Ç–æ–ø–ª–∏–≤–æ", "—Å–µ—Ä–≤–∏—Å", "–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ", "–ø–µ—Ä—Å–æ–Ω–∞–ª", "–±–∞–ª–ª—ã", "–ê–ó–°", "–±–µ–Ω–∑–∏–Ω", "–¥–∏–∑–µ–ª—å", "–∫–∞—Ä—Ç–∞", "—Ü–µ–Ω–∞",
            "–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ", "—Å–æ—Å—Ç–∞–≤", "—É–ø–∞–∫–æ–≤–∫–∞", "–∏–∫—Ä–∞"
        ]
        self.review_data = []
        self.stop_words = set(["–∏", "–≤", "–Ω–∞", "—Å", "–ø–æ", "–¥–ª—è", "–∫", "—É", "–∏–∑", "–æ", "–∞", "–Ω–æ", "–∏–ª–∏"])

    def preprocess_text(self, text: str) -> str:
        if not self.use_preprocessing or not text.strip():
            return text
        if speller is None:
            logger.warning("pyaspeller –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç–∫–ª—é—á–µ–Ω–∞.")
            return text
        try:
            emoji_dict = {
                "üëç": "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π_—ç–º–æ–¥–∑–∏", "üëå": "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π_—ç–º–æ–¥–∑–∏", "üòä": "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è",
                "üôÇ": "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è", "üòç": "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è", "üò¢": "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è",
                "üò°": "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è", "üò†": "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è", "üòã": "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è",
                "ÔºÅ": "–≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏–µ", "‚Äº": "–¥–≤–æ–π–Ω–æ–µ_–≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏–µ", "Ôºü": "–≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π_–∑–Ω–∞–∫",
                "üåª": "—Ü–≤–µ—Ç–æ–∫_—ç–º–æ–¥–∑–∏", "üöò": "–º–∞—à–∏–Ω–∞_—ç–º–æ–¥–∑–∏", "üëé": "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π_—ç–º–æ–¥–∑–∏",
                "üòû": "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è", "ü§î": "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ_—ç–º–æ—Ü–∏—è", "üí°": "–∏–¥–µ—è_—ç–º–æ–¥–∑–∏"
            }
            for emoji, label in emoji_dict.items():
                text = text.replace(emoji, f" {label} ")
            corrected = speller.spelled(text)
            typo_corrections = {"–ø—Ä–∏ —Ö–∞—Ä–∏": "–ø—Ä–∏ —Ö—Ä–∞–Ω–µ–Ω–∏–∏", "—Ö–∞—Ä–∏": "—Ö—Ä–∞–Ω–µ–Ω–∏–∏"}
            for typo, correction in typo_corrections.items():
                corrected = corrected.replace(typo, correction)
            doc = nlp(corrected)
            normalized_tokens = [
                morph.parse(token.text)[0].normal_form
                for token in doc
                if not token.is_punct and not token.is_stop and token.text.lower() not in self.stop_words
            ]
            return " ".join(normalized_tokens).strip()
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–∞: {str(e)}")
            return text

    def split_sentences(self, text: str) -> List[str]:
        text = re.sub(r'[!?.]+\)+|\)+|[:;]-?\)+', '.', text)
        text = re.sub(r'(\.+|\!+|\?+)', r'. ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        doc = nlp(text)
        return [sent.text.strip() for sent in doc.sents if sent.text.strip()]

    def split_mixed_sentence(self, sentence: str) -> List[str]:
        parts = re.split(r'\s+(–Ω–æ|–∞|–∏–ª–∏)\s+', sentence.lower())
        return [part.strip() for part in parts if part.strip()] if len(parts) > 1 else [sentence]

    def check_modifiers_with_dependencies(self, sentence: str) -> float:
        doc = nlp(sentence)
        sentiment_modifier = 1.0
        negation_count = 0
        intensifier = 1.0
        for token in doc:
            if token.lemma_ in ["–Ω–µ", "–Ω–µ—Ç", "–Ω–∏", "–µ–¥–≤–∞"] and token.dep_ in ["neg"]:
                negation_count += 1
            elif token.lemma_ in ["–æ—á–µ–Ω—å", "–∫—Ä–∞–π–Ω–µ", "—Å–∏–ª—å–Ω–æ", "–∞–±—Å–æ–ª—é—Ç–Ω–æ", "—Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ"] and token.dep_ in ["advmod"]:
                intensifier = 1.5
            elif token.lemma_ in ["—Å–ª–µ–≥–∫–∞", "–Ω–µ–º–Ω–æ–≥–æ", "—á—É—Ç—å", "–µ–ª–µ"] and token.dep_ in ["advmod"]:
                intensifier = 0.5
            elif token.lemma_ in ["–≤—Ä—è–¥ –ª–∏", "–µ–¥–≤–∞ –ª–∏"] and token.dep_ in ["advmod"]:
                negation_count += 1
        sentiment_modifier = -intensifier if negation_count % 2 == 1 else intensifier
        logger.debug(f"–ú–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è '{replace_emoji_for_logging(sentence)}': {sentiment_modifier}, negation_count={negation_count}")
        return sentiment_modifier

    def analyze_sentiment_transformers(self, text: str) -> Tuple[str, float]:
        if not text.strip():
            return '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ', 0.0
        if text in self.sentiment_cache:
            return self.sentiment_cache[text]
        if sentiment_analyzer:
            try:
                result = sentiment_analyzer(text)[0]
                label = result['label'].lower()
                score = result['score']
                logger.info(f"–ú–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É–ª–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞ '{replace_emoji_for_logging(text)}': label={label}, score={score}")
                doc = nlp(text)
                has_negative = any(token.lemma_ in [k for k, (s, _) in SENTIMENT_DICT.items() if s == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ"] for token in doc)
                has_positive = any(token.lemma_ in [k for k, (s, _) in SENTIMENT_DICT.items() if s == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ"] for token in doc)
                negative_boost = 0
                if has_negative:
                    negative_words = [token.lemma_ for token in doc if SENTIMENT_DICT.get(token.lemma_, (None, None))[0] == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ"]
                    negative_boost = -0.7 * len(negative_words) * min(abs(SENTIMENT_DICT.get(word, (None, 0.0))[1]) for word in negative_words if SENTIMENT_DICT.get(word))
                if label == "positive" and score > 0.6:
                    base_score = score
                elif label == "negative" and score > 0.6:
                    base_score = -score
                elif label == "neutral":
                    base_score = negative_boost if has_negative and negative_boost < -0.5 else 0.6 if has_positive else 0.0
                else:
                    base_score = 0.0
                modifier = self.check_modifiers_with_dependencies(text)
                adjusted_score = base_score * modifier
                if "!" in text:
                    adjusted_score *= 1.2
                elif "..." in text or "?" in text:
                    adjusted_score *= 0.8
                sentiment = "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ" if adjusted_score > 0.4 else "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ" if adjusted_score < -0.4 else "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ"
                self.sentiment_cache[text] = (sentiment, adjusted_score)
                return sentiment, adjusted_score
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏: {e}. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å–ª–æ–≤–∞—Ä–Ω—ã–π –º–µ—Ç–æ–¥.")
                return self.fallback_sentiment_analysis(text)
        return self.fallback_sentiment_analysis(text)

    def fallback_sentiment_analysis(self, text: str) -> Tuple[str, float]:
        doc = nlp(text.lower())
        sentiment_scores = {"–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ": 0, "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ": 0}
        for token in doc:
            sentiment, score = SENTIMENT_DICT.get(token.lemma_, ("–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ", 0.0))
            if sentiment in ["–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ"]:
                sentiment_scores[sentiment] += score
        modifier = self.check_modifiers_with_dependencies(text)
        total_score = (sentiment_scores["–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ"] + sentiment_scores["–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ"]) * modifier
        return ("–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ", total_score) if total_score > 0.4 else ("–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ", total_score) if total_score < -0.4 else ("–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ", 0.0)

    def extract_aspects(self, sentence: str) -> List[Tuple[str, str, float, str]]:
        doc = nlp(sentence)
        aspects = []
        invalid_words = {"–æ—á–µ–Ω–∫", "–ø–æ—Ä–æ–±–æ–≤–∞—Ç—å", "—Å–ø–∞—Å–Ω–æ–≥–æ", "–∑–∞—Å–∫—É—á–∞—Ç—å—Å—è", "–¥–æ–±–æ–≤—Å—Ç–≤–æ", "—Ö–∞—Ä–∏"}
        MAX_ASPECT_LENGTH = 5

        if sentence.lower().strip() in ["–Ω–µ—Ç", "–Ω–µ—Ç—É"]:
            sentiment, score = self.get_sentiment("–Ω–µ—Ç")
            aspects.append(("–Ω–µ—Ç", sentiment, score, sentence))
            logger.info(f"–ò–∑–≤–ª–µ—á—ë–Ω –∞—Å–ø–µ–∫—Ç '–Ω–µ—Ç': —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å={sentiment}, —Å–∫–æ—Ä={score}")
            return aspects

        sentiment, score = self.analyze_sentiment_transformers(sentence)
        clauses = self.split_mixed_sentence(sentence)

        for clause in clauses:
            doc_clause = nlp(clause)
            for token in doc_clause:
                if (token.pos_ in ["NOUN", "ADJ", "VERB"] and token.lemma_ not in invalid_words and 
                    (token.lemma_ in self.domain_hints or token.lemma_ in [k for k, _ in SENTIMENT_DICT.items()])):
                    modifiers = []
                    negation = False
                    children_count = 0
                    for child in token.children:
                        if child.dep_ in ["amod", "compound", "advmod", "nmod", "obj"] and child.lemma_ not in invalid_words and children_count < MAX_ASPECT_LENGTH - 1:
                            modifiers.append(child.lemma_)
                            children_count += 1
                        if child.lemma_ in ["–Ω–µ", "–Ω–µ—Ç", "–Ω–∏", "–µ–¥–≤–∞"] and child.dep_ in ["neg"]:
                            negation = True
                    if token.dep_ in ["amod", "nmod", "obj"]:
                        parent = token.head
                        if parent.pos_ in ["NOUN", "VERB"] and parent.lemma_ not in invalid_words:
                            modifiers.append(parent.lemma_)
                    aspect_phrase = " ".join(sorted([mod for mod in modifiers if mod] + [token.lemma_])).strip()
                    if not aspect_phrase or aspect_phrase.lower() in self.invalid_phrases or len(aspect_phrase.split()) > MAX_ASPECT_LENGTH:
                        continue
                    aspect_sentiment = sentiment
                    aspect_score = score
                    if negation or any(mod in [k for k, (s, _) in SENTIMENT_DICT.items() if s == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ"] for mod in modifiers):
                        aspect_sentiment = "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ"
                        aspect_score = -abs(aspect_score) if aspect_score > 0 else aspect_score
                    elif any(mod in [k for k, (s, _) in SENTIMENT_DICT.items() if s == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ"] for mod in modifiers):
                        aspect_sentiment = "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ"
                        aspect_score = abs(aspect_score)
                    aspects.append((aspect_phrase, aspect_sentiment, aspect_score, clause))
                    logger.info(f"–ò–∑–≤–ª–µ—á—ë–Ω –∞—Å–ø–µ–∫—Ç: '{replace_emoji_for_logging(aspect_phrase)}', —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {aspect_sentiment}, —Å–∫–æ—Ä: {aspect_score}, –∏–∑ —Ç–µ–∫—Å—Ç–∞: '{clause}'")
            if not aspects and sentiment != "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ" and len(sentence.split()) <= MAX_ASPECT_LENGTH:
                aspects.append((sentence.strip(), sentiment, score, sentence))
                logger.info(f"–ò–∑–≤–ª–µ—á—ë–Ω –∞—Å–ø–µ–∫—Ç (–∏–∑ –≤—Å–µ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è): '{replace_emoji_for_logging(sentence.strip())}', —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {sentiment}, —Å–∫–æ—Ä: {score}")
        return aspects

    def analyze_review_sentences(self, review_text: str) -> List[Tuple[str, str, float, List[Tuple[str, str, float, str]]]]:
        review_text = self.preprocess_text(review_text)
        sentences = self.split_sentences(review_text)
        result = []
        for sentence in sentences:
            try:
                doc = nlp(sentence)
                clauses = []
                current_clause = []
                for token in doc:
                    current_clause.append(token.text)
                    if token.dep_ in ["cc", "punct"] and token.lemma_ in ["–∏", "–Ω–æ", "–∞", "–∏–ª–∏", ","]:
                        if current_clause:
                            clauses.append(" ".join(current_clause).strip())
                            current_clause = []
                if current_clause:
                    clauses.append(" ".join(current_clause).strip())
                for clause in clauses:
                    sentiment, score = self.analyze_sentiment_transformers(clause)
                    aspects = self.extract_aspects(clause)
                    result.append((clause, sentiment, score, aspects))
                    logger.info(f"–ê–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è '{replace_emoji_for_logging(clause)}': —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å={sentiment}, —Å–∫–æ—Ä={score}, –∞—Å–ø–µ–∫—Ç—ã={aspects}")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è '{replace_emoji_for_logging(sentence)}': {str(e)}")
                result.append((sentence, "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ", 0.0, []))
        return result

    def analyze_reviews(self, csv_path: str) -> Dict[str, Union[str, Counter]]:
        try:
            df = pd.read_csv(csv_path, encoding='utf-8')
            positive_count = negative_count = neutral_count = 0
            positive_aspects = Counter()
            negative_aspects = Counter()
            self.positive_keywords.clear()
            self.negative_keywords.clear()
            self.review_data = []
            processed_texts_set = set()

            if all(col in df.columns for col in ['–î–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤–∞', '–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏', '–û—Ü–µ–Ω–∫–∞']):
                texts = [(str(row['–î–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤–∞']) if pd.notna(row['–î–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤–∞']) else "",
                         str(row['–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏']) if pd.notna(row['–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏']) else "",
                         int(row['–û—Ü–µ–Ω–∫–∞']) if pd.notna(row['–û—Ü–µ–Ω–∫–∞']) else 3,
                         str(row.get('–ò–º—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è', '–ê–Ω–æ–Ω–∏–º')))
                        for _, row in df.iterrows()]
                for pros_text, cons_text, rating, username in texts:
                    self.review_data.append({
                        'username': username,
                        'pros': pros_text,
                        'cons': cons_text,
                        'rating': rating,
                        'original_pros': pros_text,
                        'original_cons': cons_text
                    })
                    if pros_text.strip():
                        if pros_text.strip().lower() in ["–Ω–µ—Ç", "–Ω–µ—Ç—É"]:
                            negative_aspects["–Ω–µ—Ç"] += 1
                            self.negative_keywords.add("–Ω–µ—Ç")
                        else:
                            processed_texts_set.add(pros_text)
                            pros_sentences = self.analyze_review_sentences(pros_text)
                            for _, sentiment, _, aspects in pros_sentences:
                                if aspects:
                                    for aspect_phrase, aspect_sentiment, _, _ in aspects:
                                        if aspect_sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ":
                                            positive_aspects[aspect_phrase] += 1
                                            self.positive_keywords.add(aspect_phrase)
                                        elif aspect_sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ":
                                            negative_aspects[aspect_phrase] += 1
                                            self.negative_keywords.add(aspect_phrase)
                    if cons_text.strip():
                        if cons_text.strip().lower() in ["–Ω–µ—Ç", "–Ω–µ—Ç—É"]:
                            positive_aspects["–Ω–µ—Ç –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–æ–≤"] += 1
                            self.positive_keywords.add("–Ω–µ—Ç –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–æ–≤")
                        else:
                            processed_texts_set.add(cons_text)
                            cons_sentences = self.analyze_review_sentences(cons_text)
                            for _, sentiment, _, aspects in cons_sentences:
                                if aspects:
                                    for aspect_phrase, aspect_sentiment, _, _ in aspects:
                                        if aspect_sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ":
                                            positive_aspects[aspect_phrase] += 1
                                            self.positive_keywords.add(aspect_phrase)
                                        elif aspect_sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ":
                                            negative_aspects[aspect_phrase] += 1
                                            self.negative_keywords.add(aspect_phrase)
                    if rating >= self.positive_threshold:
                        positive_count += 1
                    elif rating <= self.negative_threshold:
                        negative_count += 1
                    else:
                        neutral_count += 1
            elif all(col in df.columns for col in ['–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞', '–û—Ü–µ–Ω–∫–∞']):
                texts = [(str(row['–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞']), int(row['–û—Ü–µ–Ω–∫–∞']) if pd.notna(row['–û—Ü–µ–Ω–∫–∞']) else 3,
                         str(row.get('–ò–º—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è', '–ê–Ω–æ–Ω–∏–º')))
                        for _, row in df.iterrows()]
                for text, rating, username in texts:
                    self.review_data.append({
                        'username': username,
                        'pros': text,
                        'cons': '',
                        'rating': rating,
                        'original_pros': text,
                        'original_cons': ''
                    })
                    if text.strip():
                        processed_texts_set.add(text)
                        sentences = self.analyze_review_sentences(text)
                        for _, sentiment, _, aspects in sentences:
                            if aspects:
                                for aspect_phrase, aspect_sentiment, _, _ in aspects:
                                    if aspect_sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ":
                                        positive_aspects[aspect_phrase] += 1
                                        self.positive_keywords.add(aspect_phrase)
                                    elif aspect_sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ":
                                        negative_aspects[aspect_phrase] += 1
                                        self.negative_keywords.add(aspect_phrase)
                    if rating >= self.positive_threshold:
                        positive_count += 1
                    elif rating <= self.negative_threshold:
                        negative_count += 1
                    else:
                        neutral_count += 1
            else:
                raise ValueError("CSV-—Ñ–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –ª–∏–±–æ —Å—Ç–æ–ª–±—Ü—ã '–î–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤–∞', '–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏' –∏ '–û—Ü–µ–Ω–∫–∞', –ª–∏–±–æ '–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞' –∏ '–û—Ü–µ–Ω–∫–∞'")

            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–µ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –∞—Å–ø–µ–∫—Ç–æ–≤
            for aspect in list(positive_aspects.keys()):
                if aspect.lower() in ["–Ω–µ—Ç –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–æ–≤", "–Ω–µ—Ç"]:
                    del positive_aspects[aspect]
                    self.positive_keywords.discard(aspect)
            for aspect in list(negative_aspects.keys()):
                if aspect.lower() in ["–Ω–µ—Ç", "–Ω–µ—Ç—É"]:
                    del negative_aspects[aspect]
                    self.negative_keywords.discard(aspect)

            # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è –∞—Å–ø–µ–∫—Ç–æ–≤
            common_aspects = set(positive_aspects.keys()).intersection(set(negative_aspects.keys()))
            for aspect in common_aspects:
                sentiment, _ = self.analyze_sentiment_transformers(aspect)
                if sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ":
                    del negative_aspects[aspect]
                    self.negative_keywords.discard(aspect)
                elif sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ":
                    del positive_aspects[aspect]
                    self.positive_keywords.discard(aspect)
                else:
                    pos_count = positive_aspects[aspect]
                    neg_count = negative_aspects[aspect]
                    if pos_count > neg_count:
                        del negative_aspects[aspect]
                        self.negative_keywords.discard(aspect)
                    else:
                        del positive_aspects[aspect]
                        self.positive_keywords.discard(aspect)

            main_positives = "\n".join([f"{aspect} ({count})" for aspect, count in positive_aspects.most_common(5)]) if positive_aspects else "–ù–µ—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤"
            main_negatives = "\n".join([f"{aspect} ({count})" for aspect, count in negative_aspects.most_common(5)]) if negative_aspects else "–ù–µ—Ç –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤"

            return {
                "–ü–ª—é—Å—ã": main_positives,
                "–ú–∏–Ω—É—Å—ã": main_negatives,
                "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ)": ", ".join(sorted(self.positive_keywords)[:10]),
                "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ)": ", ".join(sorted(self.negative_keywords)[:10]),
                "–û–±—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞": ", ".join(sorted(set(self.positive_keywords).union(self.negative_keywords))[:15]),
                "–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": str(positive_count),
                "–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": str(negative_count),
                "–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": str(neutral_count),
                "positive_aspects": positive_aspects,
                "negative_aspects": negative_aspects
            }
        except UnicodeDecodeError as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ {csv_path}: {str(e)}")
            return {
                "–ü–ª—é—Å—ã": "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏",
                "–ú–∏–Ω—É—Å—ã": "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏",
                "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ)": "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏",
                "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ)": "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏",
                "–û–±—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞": "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏",
                "–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏",
                "–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏",
                "–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏",
                "positive_aspects": Counter(),
                "negative_aspects": Counter()
            }
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Ñ–∞–π–ª–∞ {csv_path}: {str(e)}")
            return {
                "–ü–ª—é—Å—ã": "–û—à–∏–±–∫–∞",
                "–ú–∏–Ω—É—Å—ã": "–û—à–∏–±–∫–∞",
                "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ)": "–û—à–∏–±–∫–∞",
                "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ)": "–û—à–∏–±–∫–∞",
                "–û–±—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞": "–û—à–∏–±–∫–∞",
                "–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": "–û—à–∏–±–∫–∞",
                "–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": "–û—à–∏–±–∫–∞",
                "–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã": "–û—à–∏–±–∫–∞",
                "positive_aspects": Counter(),
                "negative_aspects": Counter()
            }

    def aggregate_reviews(self, csv_paths: List[str]) -> Dict[str, Union[str, Counter]]:
        all_keywords: List[str] = []
        total_positive_count = total_negative_count = total_neutral_count = 0
        all_positive_aspects = Counter()
        all_negative_aspects = Counter()

        with ThreadPoolExecutor(max_workers=8) as executor:
            results = list(executor.map(self.analyze_reviews, csv_paths))

        for result in results:
            if result["–ü–ª—é—Å—ã"] not in ["–û—à–∏–±–∫–∞", "–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏"]:
                positive_count = int(result["–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã"])
                negative_count = int(result["–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã"])
                neutral_count = int(result["–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã"])
                total_positive_count += positive_count
                total_negative_count += negative_count
                total_neutral_count += neutral_count
                all_positive_aspects.update(result["positive_aspects"])
                all_negative_aspects.update(result["negative_aspects"])
                all_keywords.extend(result["–û–±—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞"].split(", "))

        main_positives = "\n".join([f"{aspect} ({count})" for aspect, count in all_positive_aspects.most_common(5)]) if all_positive_aspects else "–ù–µ—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤"
        main_negatives = "\n".join([f"{aspect} ({count})" for aspect, count in all_negative_aspects.most_common(5)]) if all_negative_aspects else "–ù–µ—Ç –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤"
        top_keywords = ", ".join(sorted(set(all_keywords))[:15]) if all_keywords else "–ù–µ—Ç –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"

        return {
            "–ü–ª—é—Å—ã (–≤—Å–µ —Å–∞–π—Ç—ã)": main_positives,
            "–ú–∏–Ω—É—Å—ã (–≤—Å–µ —Å–∞–π—Ç—ã)": main_negatives,
            "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ, –≤—Å–µ —Å–∞–π—Ç—ã)": ", ".join(sorted(self.positive_keywords)[:10]),
            "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ, –≤—Å–µ —Å–∞–π—Ç—ã)": ", ".join(sorted(self.negative_keywords)[:10]),
            "–û–±—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–≤—Å–µ —Å–∞–π—Ç—ã)": top_keywords,
            "–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã (–≤—Å–µ —Å–∞–π—Ç—ã)": str(total_positive_count),
            "–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã (–≤—Å–µ —Å–∞–π—Ç—ã)": str(total_negative_count),
            "–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã (–≤—Å–µ —Å–∞–π—Ç—ã)": str(total_neutral_count),
            "positive_aspects": all_positive_aspects,
            "negative_aspects": all_negative_aspects
        }

    def get_sentiment(self, word: str) -> Tuple[str, float]:
        return SENTIMENT_DICT.get(word, ("–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ", 0.0))

    def update_sentiment_dict(self, word: str, sentiment: str, score: float):
        if word in SENTIMENT_DICT:
            current_sentiment, current_score = SENTIMENT_DICT[word]
            new_score = (current_score + score) / 2
            SENTIMENT_DICT[word] = (sentiment, new_score)
        else:
            SENTIMENT_DICT[word] = (sentiment, score)
        logger.info(f"–û–±–Ω–æ–≤–ª—ë–Ω —Å–ª–æ–≤–∞—Ä—å —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏: {word} -> {SENTIMENT_DICT[word]}")

    def find_representative_examples(self, aspect: str, aspect_sentiment: str, max_examples: int = 2) -> List[str]:
        examples = []
        aspect_words = set(aspect.lower().split())
        positive_examples = []
        negative_examples = []

        for review in self.review_data:
            pros_text = review['original_pros'].lower() if review['original_pros'] else ''
            cons_text = review['original_cons'].lower() if review['original_cons'] else ''
            review_text = pros_text + ' ' + cons_text

            if not all(word in review_text for word in aspect_words):
                continue

            sentiment, _ = self.analyze_sentiment_transformers(review_text)
            example_text = review['original_pros'] if pros_text else review['original_cons']
            example = f'"{example_text}" ({review["username"]})'

            if sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ":
                positive_examples.append(example)
            elif sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ":
                negative_examples.append(example)

        if aspect_sentiment == "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ":
            examples.extend(positive_examples[:max_examples])
            if len(examples) < max_examples and negative_examples:
                examples.append(negative_examples[0])
        elif aspect_sentiment == "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ":
            examples.extend(negative_examples[:max_examples])
            if len(examples) < max_examples and positive_examples:
                examples.append(positive_examples[0])
        return examples[:max_examples]

    def generate_detailed_report(self, analysis_result: Dict[str, Union[str, Counter]], product_name: str = "–ø—Ä–æ–¥—É–∫—Ç") -> str:
        positive_aspects = analysis_result["positive_aspects"]
        negative_aspects = analysis_result["negative_aspects"]
        positive_count = int(analysis_result["–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã"])
        negative_count = int(analysis_result["–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã"])
        neutral_count = int(analysis_result["–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã"])
        total_reviews = positive_count + negative_count + neutral_count

        current_time = datetime.now().strftime("%d.%m.%Y %H:%M")
        report = f"–û—Ç—á—ë—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –æ—Ç–∑—ã–≤–æ–≤\n–ê–≤—Ç–æ—Ä: –ù–∏–∫–∏—Ç–∞ –ß–µ–ª—ã—à–µ–≤\n–î–∞—Ç–∞: {current_time}\n–ü—Ä–æ–¥—É–∫—Ç: {product_name}\n\n"
        report += "=" * 50 + "\n\n"

        report += "1. –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞\n"
        report += "-" * 30 + "\n"
        if not positive_aspects:
            report += "–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç.\n\n"
        else:
            for aspect, count in positive_aspects.most_common(5):
                examples = self.find_representative_examples(aspect, "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ")
                formatted_aspect = ' '.join(word.capitalize() for word in aspect.split())
                report += f"- {formatted_aspect} ({count} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π)\n"
                report += f"  –ü—Ä–∏–º–µ—Ä—ã: {', '.join(examples) if examples else '–ü—Ä–∏–º–µ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.'}\n\n"

        report += "2. –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏\n"
        report += "-" * 30 + "\n"
        if not negative_aspects:
            report += "–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç.\n\n"
        else:
            for aspect, count in negative_aspects.most_common(5):
                examples = self.find_representative_examples(aspect, "–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ")
                formatted_aspect = ' '.join(word.capitalize() for word in aspect.split())
                report += f"- {formatted_aspect} ({count} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π)\n"
                report += f"  –ü—Ä–∏–º–µ—Ä—ã: {', '.join(examples) if examples else '–ü—Ä–∏–º–µ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.'}\n\n"

        report += "3. –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞\n"
        report += "-" * 30 + "\n"
        report += f"–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ: {', '.join(sorted(self.positive_keywords)[:10])}\n"
        report += f"–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ: {', '.join(sorted(self.negative_keywords)[:10])}\n"
        report += f"–û–±—â–∏–µ: {analysis_result['–û–±—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞']}\n\n"

        report += "4. –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n"
        report += "-" * 30 + "\n"
        positive_percentage = (positive_count / total_reviews * 100) if total_reviews > 0 else 0
        negative_percentage = (negative_count / total_reviews * 100) if total_reviews > 0 else 0
        neutral_percentage = (neutral_count / total_reviews * 100) if total_reviews > 0 else 0
        average_rating = sum(review['rating'] for review in self.review_data) / total_reviews if total_reviews > 0 else 0
        report += f"–í—Å–µ–≥–æ –æ—Ç–∑—ã–≤–æ–≤: {total_reviews}\n"
        report += f"–°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞: {average_rating:.1f}/5\n"
        report += f"–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã (4-5): {positive_count} ({positive_percentage:.1f}%)\n"
        report += f"–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã (1-2): {negative_count} ({negative_percentage:.1f}%)\n"
        report += f"–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã (3): {neutral_count} ({neutral_percentage:.1f}%)\n\n"

        report += "5. –û–±—â–µ–µ –≤–ø–µ—á–∞—Ç–ª–µ–Ω–∏–µ\n"
        report += "-" * 30 + "\n"
        top_positive_aspects = [aspect for aspect, _ in positive_aspects.most_common(3)]
        top_negative_aspects = [aspect for aspect, _ in negative_aspects.most_common(3)]
        if positive_percentage > negative_percentage:
            report += f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –≤ —Ü–µ–ª–æ–º –¥–æ–≤–æ–ª—å–Ω—ã –ø—Ä–æ–¥—É–∫—Ç–æ–º '{product_name}'.\n"
            report += f"–û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞: {', '.join(top_positive_aspects).lower()}.\n"
            report += f"–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã —Å–æ—Å—Ç–∞–≤–ª—è—é—Ç {positive_percentage:.1f}%.\n"
            if top_negative_aspects:
                report += f"–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏: {', '.join(top_negative_aspects).lower()} ({negative_percentage:.1f}% –æ—Ç–∑—ã–≤–æ–≤).\n"
        else:
            report += f"–í–ø–µ—á–∞—Ç–ª–µ–Ω–∏—è –æ –ø—Ä–æ–¥—É–∫—Ç–µ '{product_name}' —Å–º–µ—à–∞–Ω–Ω—ã–µ.\n"
            report += f"–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞: {', '.join(top_positive_aspects).lower()} ({positive_percentage:.1f}% –æ—Ç–∑—ã–≤–æ–≤).\n"
            report += f"–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏: {', '.join(top_negative_aspects).lower()} ({negative_percentage:.1f}% –æ—Ç–∑—ã–≤–æ–≤).\n"

        report += "\n6. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n"
        report += "-" * 30 + "\n"
        report += f"–°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ –ø—Ä–æ–¥—É–∫—Ç–∞: {average_rating:.1f}/5.\n"
        if average_rating >= 4.0:
            report += f"–ü—Ä–æ–¥—É–∫—Ç –ø–æ–ª—É—á–∞–µ—Ç –≤—ã—Å–æ–∫—É—é –æ—Ü–µ–Ω–∫—É.\n"
        elif average_rating >= 3.0:
            report += f"–ü—Ä–æ–¥—É–∫—Ç –≤–æ—Å–ø—Ä–∏–Ω–∏–º–∞–µ—Ç—Å—è —É–º–µ—Ä–µ–Ω–Ω–æ.\n"
        else:
            report += f"–ü—Ä–æ–¥—É–∫—Ç –∏–º–µ–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã.\n"
        if negative_aspects:
            top_negatives = [aspect for aspect, _ in negative_aspects.most_common(2)]
            report += f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–ª—É—á—à–∏—Ç—å: {', '.join(top_negatives).lower()}.\n"

        report += "\n" + "=" * 50 + "\n"

        # –≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        result_data = {
            "positive_aspects": dict(positive_aspects),
            "negative_aspects": dict(negative_aspects),
            "positive_count": positive_count,
            "negative_count": negative_count,
            "neutral_count": neutral_count,
            "average_rating": average_rating,
            "positive_keywords": list(self.positive_keywords),
            "negative_keywords": list(self.negative_keywords)
        }
        with open("analysis_results.json", "w", encoding="utf-8") as f:
            json.dump(result_data, f, ensure_ascii=False, indent=4)

        return report

